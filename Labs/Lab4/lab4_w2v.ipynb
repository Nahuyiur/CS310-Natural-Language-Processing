{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 4 (part 2): Data preparation for implementing word2vec\n",
    "\n",
    "skipgram architecture and negative sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pprint import pprint\n",
    "from utils import CorpusReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n"
     ]
    }
   ],
   "source": [
    "# We set min_count=1 to include all words in the corpus\n",
    "corpus = CorpusReader(inputFileName=\"lunyu_20chapters.txt\", min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "子\n",
      "1352\n"
     ]
    }
   ],
   "source": [
    "print(corpus.word2id[\"子\"])\n",
    "print(corpus.id2word[1])\n",
    "print(len(corpus.id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient way for negative sampling\n",
    "\n",
    "In `utils.CorpusReader` class, we have implemented a method `initTableNegatives`. It creates a list of words (`self.negatives`) with a size of 1e8. This size is set a large value so that it scales up to very large corpus. \n",
    "\n",
    "The list contains the index of each word in the vocabulary, whose probability is proportional to the power of 0.75 of the word's original frequency count. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simulation of how initTableNegatives works\n",
    "# The impl. in utils.py is a bit different, but the idea is the same\n",
    "word_frequency = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n",
    "\n",
    "# the scaled sum of frequencies Z = 1**0.75 + 2**0.75 + 3**0.75 + 4**0.75 = 7.7897270\n",
    "# then the scaled probability of a = 1**0.75 / Z = 0.12837420128374202\n",
    "# the scaled probability of b = 2**0.75 / Z = 0.21589881215898812\n",
    "# the scaled probability of c = 3**0.75 / Z = 0.29262990292629903\n",
    "# the scaled probability of d = 4**0.75 / Z = 0.3630970836309708\n",
    "\n",
    "def initTableNegatives():\n",
    "    pow_frequency = np.array(list(word_frequency.values())) ** 0.75 # 每个词的频率，并对其进行 0.75次方\n",
    "    words_pow = sum(pow_frequency)\n",
    "    ratio = pow_frequency / words_pow # 计算每个词的缩放概率\n",
    "    count = np.round(ratio * CorpusReader.NEGATIVE_TABLE_SIZE) # 计算每个词在负样本表中的出现次数\n",
    "    negatives = []\n",
    "    for wid, c in enumerate(count): # wid 是每个词的索引，c 是词在负样本表中的出现次数\n",
    "        negatives += [wid] * int(c) # 把索引按次数添加到negatives中\n",
    "    negatives = np.array(negatives)\n",
    "    np.random.shuffle(negatives)\n",
    "    return negatives\n",
    "\n",
    "negatives = initTableNegatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999999\n",
      "{0, 1, 2, 3}\n",
      "0.12837420128374202\n",
      "0.21589881215898812\n",
      "0.29262990292629903\n",
      "0.3630970836309708\n"
     ]
    }
   ],
   "source": [
    "print(len(negatives))\n",
    "print(set(negatives)) # the word indices: a -> 0, b -> 1, c -> 2, d -> 3\n",
    "print(np.sum(negatives == 0) / len(negatives)) # should be the scaled probability of a\n",
    "print(np.sum(negatives == 1) / len(negatives)) # should be the scaled probability of b\n",
    "print(np.sum(negatives == 2) / len(negatives)) # should be the scaled probability of c\n",
    "print(np.sum(negatives == 3) / len(negatives)) # should be the scaled probability of d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `getNegatives` method returns the negative samples for a target word. The idea is to chop off a segment of given `size` from the `negatives` list. \n",
    "\n",
    "If the segment contains the target word, it is discarded and a new segment is taken. This is done to avoid the target word itself to be sampled as a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([207, 344, 447,   0,  26])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test some examples\n",
    "corpus.getNegatives(target=1, size=5)\n",
    "# 从已经构建好的负样本表中，随机返回一定数量（由 size 参数指定）的负样本；\n",
    "# 如果选定了目标词（target），丢弃这个段，重新选择一段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Generate data for training\n",
    "\n",
    "Now we are going to implement the sliding window to generate center, outside, and negative words for each position in a sentence.\n",
    "\n",
    "- It takes a list of words as input and go through each word as a center word.\n",
    "- For each center word, both the left and right `window_size` words are considered as outside words. This number is smaller near the two ends of the sentence.\n",
    "- Call `corpus.getNegatives` to get negative samples for each center word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(words: List[str], window_size: int, k: int, corpus: CorpusReader):\n",
    "    \"\"\" Generate the training data for word2vec skip-gram model\n",
    "    Args:\n",
    "        text: the input text\n",
    "        window_size: the size of the context window\n",
    "        k: the number of negative samples\n",
    "        corpus: the corpus object, providing utilities such as word2id, getNegatives, etc.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    word_ids = [corpus.word2id[word] for word in words] # 把word转换成id\n",
    "    # 这个word_ids是一个列表\n",
    "    \n",
    "    for i,central_word_id in enumerate(word_ids):\n",
    "        # 窗格的范围\n",
    "        start=max(0,i-window_size)\n",
    "        end=min(len(word_ids),i+window_size+1)# 在下面的循环中，这个不含\n",
    "\n",
    "        outside_word_ids=[word_ids[j] for j in range(start,end) if i!=j]\n",
    "        \n",
    "        for outside_word_id in outside_word_ids:\n",
    "            negative_samples=corpus.getNegatives(target=central_word_id,size=k)\n",
    "            # 生成训练数据，格式为(center_word, outside_word, negative_samples)\n",
    "            yield(central_word_id,outside_word_id,negative_samples)\n",
    "    # Use for loop and yield\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['学', '而', '时', '习', '之']\n",
      "word ids: [46, 8, 224, 544, 5]\n",
      "\n",
      "When window size is 3, for center word 学 -> 46\n",
      "the outside words are: \n",
      "而 -> 8\n",
      "时 -> 224\n",
      "习 -> 544\n",
      "\n",
      "output from generate_data:\n",
      "[(46, 8, array([173,  17, 814, 185, 230])),\n",
      " (46, 224, array([  41,  273, 1165,  111,  457])),\n",
      " (46, 544, array([  0, 433, 444, 230,   2]))]\n"
     ]
    }
   ],
   "source": [
    "# Test generate_data\n",
    "text = \"学而时习之\"\n",
    "words = list(text)\n",
    "print('words:', words)\n",
    "print('word ids:', [corpus.word2id[word] for word in words])\n",
    "\n",
    "# first center word is 学\n",
    "print()\n",
    "print(f'When window size is 3, for center word 学 -> {corpus.word2id[\"学\"]}')\n",
    "print(f'the outside words are: ')\n",
    "print(f'而 -> {corpus.word2id[\"而\"]}')\n",
    "print(f'时 -> {corpus.word2id[\"时\"]}')\n",
    "print(f'习 -> {corpus.word2id[\"习\"]}')\n",
    "\n",
    "print()\n",
    "print('output from generate_data:')\n",
    "data = list(generate_data(list(text), window_size=3, k=5, corpus=corpus))\n",
    "pprint(data[:3])\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# words: ['学', '而', '时', '习', '之']\n",
    "# word ids: [46, 8, 224, 544, 5]\n",
    "\n",
    "# When window size is 3, for center word 学 -> 46\n",
    "# the outside words are: \n",
    "# 而 -> 8\n",
    "# 时 -> 224\n",
    "# 习 -> 544\n",
    "\n",
    "# output from generate_data:\n",
    "# [(46, 8, array([354,   3, 831, 570,  27])),\n",
    "#  (46, 224, array([1077, 1095,   89,  340,   92])),\n",
    "#  (46, 544, array([ 49, 488,   4, 269,  30]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above data are not in batch. We want all center words are batched into a tensor of dimension `batch_size`; same for the outside words and negative samples.\n",
    "\n",
    "For example, in \"学而时习之\", if `batch_size` is 4, then the returned batch[0] will contain three tensors. \n",
    "- The first tensor contains center words, i.e., 3 \"学\" plus 1 \"而\" => [46, 46, 46, 8]\n",
    "- The second tensor contains the correponding outside words, i.e., \"而\", \"时\", and \"习\" for \"学\"; \"学\" for \"而\" => [8, 224, 544,  46]\n",
    "- The third tensor contains the negative samples, whose dimension is `batch_size` $\\times$ `k`\n",
    "  \n",
    "The data type of the tensors is `torch.long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: List, batch_size: int):\n",
    "    \"\"\" Group a stream into batches and yield them as torch tensors.\n",
    "    Args:\n",
    "        data: a list of tuples\n",
    "        batch_size: the batch size \n",
    "    Yields:\n",
    "        a tuple of three torch tensors: center, outside, negative\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data) # data should be long enough\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        if i > len(data) - batch_size: # if the last batch is smaller than batch_size, pad it with the first few data\n",
    "            batch = batch + data[:i + batch_size - len(data)]\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        center=[]\n",
    "        outside=[]\n",
    "        negative=[]\n",
    "\n",
    "        for center_word,outside_word,neg_samples in batch:\n",
    "            center.append(center_word)\n",
    "            outside.append(outside_word)\n",
    "            negative.append(neg_samples)\n",
    "\n",
    "        center_tensor=torch.tensor(center,dtype=torch.long)\n",
    "        outside_tensor=torch.tensor(outside,dtype=torch.long)\n",
    "        negative_tensor=torch.tensor(negative,dtype=torch.long)\n",
    "\n",
    "        yield(center_tensor,outside_tensor,negative_tensor)\n",
    "        ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([46, 46, 46,  8]), tensor([  8, 224, 544,  46]), tensor([[  75,   44,  134,  267,   58],\n",
      "        [ 137,   84,    0,  505,    4],\n",
      "        [   2,  357,    1,  209,  559],\n",
      "        [  49,   19,    3,   78, 1085]]))\n"
     ]
    }
   ],
   "source": [
    "# Test batchify\n",
    "\n",
    "text = \"学而时习之\"\n",
    "words = list(text)\n",
    "data = list(generate_data(words, window_size=3, k=5, corpus=corpus))\n",
    "\n",
    "batches = list(batchify(data, batch_size=4))\n",
    "print(batches[0])\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# (tensor([46, 46, 46,  8]), tensor([  8, 224, 544,  46]), tensor([[  85,    3,   72,   26,   35],\n",
    "#         [   7,    1,  487,   20,    4],\n",
    "#         [  12,  227,    2,   25,  639],\n",
    "#         [ 582,  148,   15, 1203,   85]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Implement the SkipGram class\n",
    "\n",
    "`SkipGram` is a subclass of `nn.Module`. The two key components are:\n",
    "- `__init__`: initialize the embeddings\n",
    "  - Two `nn.Embedding` objects are created: `self.emb_v` for center words; `self.emb_u` for outside words and negative samples.\n",
    "  - Each `nn.Embedding` is created with `vocab_size` and `emb_dim` as input arguments. \n",
    "  - `self.emb_v` is initialized with uniform distribution; `self.emb_u` is initialized with zeros.\n",
    "- `forward`: given input tensors, return the loss of the model\n",
    "  - Takes three tensors as input: center words, outside words, and negative samples. They are the output from the previously defined `batchify` function.\n",
    "  - Compute the loss using the formula: $-\\log\\sigma(v_c \\cdot u_o) - \\sum_{k=1}^K \\log\\sigma(-v_c \\cdot u_k)$\n",
    "\n",
    "*Hint*:\n",
    "- For the $\\log\\sigma$ function, you can use `F.logsigmoid` in PyTorch. See the imported module: `import torch.nn.functional as F`\n",
    "- If the input to `F.logsigmoid` is too large, it will return 0, which is not good for training. You can use `torch.clamp` to limit the input to a certain range. For example, `torch.clamp(x, min=-10, max=10)` will limit the input to be in the range of $[-10, 10]$.\n",
    "\n",
    "SkipGram模型是一个用于训练词向量的经典模型，通常在Word2Vec算法中使用。其目标是通过给定的中心词预测周围词（即上下文词）。它的核心思想是，给定一个中心词，模型通过预测其上下文词来学习词嵌入（word embeddings）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        # 创建中心词的嵌入层\n",
    "        self.emb_v = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        # 创建外部词和负样本的嵌入层\n",
    "        self.emb_u = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_size # some experience passed down from generation to generation\n",
    "        # self.emb_v使用均匀分布初始化。这意味着每个词的初始嵌入值是通过均匀分布随机生成的。\n",
    "        nn.init.uniform_(self.emb_v.weight.data, -initrange, initrange) # same outcome as self.emb_v.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.emb_u则用零初始化。这是因为我们希望上下文词的嵌入在开始时并没有显著的权重影响\n",
    "        nn.init.constant_(self.emb_u.weight.data, 0) # same outcome as self.emb_u.weight.data.zero_()\n",
    "\n",
    "    def forward(self, center, outside, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center: the center word indices (B, ) B是batch size\n",
    "            outside: the outside word indices (B, )\n",
    "            negative: the negative word indices (B, k)\n",
    "        \"\"\"\n",
    "        v_c = self.emb_v(center) # 维度：(batch_size, emb_dim)\n",
    "        u_o = self.emb_u(outside)# 维度：(batch_size, emb_dim)\n",
    "        u_n = self.emb_u(negative)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # 计算正样本得分：中心词和外部词的点积\n",
    "        positive_score=torch.sum(v_c*u_o,dim=1) # 逐元素相乘（即Hadamard乘积），依然是 (batch_size, emb_dim)\n",
    "        #但是点乘完了，做的torch.sum(..., dim=1)：在第二个维度（即每个词向量的维度）上进行求和\n",
    "        #positive_loss是每个样本的损失，维度为 (batch_size,)；也就是每个样本都有一个loss\n",
    "\n",
    "        positive_score=torch.clamp(positive_score,min=-10,max=10)# 限制正样本得分\n",
    "        postive_loss=-F.logsigmoid(positive_score)\n",
    "\n",
    "        #计算负样本得分：中心词和负样本的点积\n",
    "        #unsqueeze(1) 操作将 v_c 的维度从 (batch_size, emb_dim) 转换为 (batch_size, 1, emb_dim)；把v_c 转换成一个三维张量，方便与其他三维张量进行批量矩阵乘法\n",
    "        #u_n.transpose(1, 2) 将 u_n 的维度从 (batch_size, K, emb_dim) 转置为 (batch_size, emb_dim, K)；从第二维和第三维交换\n",
    "        #然后做的是批量矩阵乘法：(batch_size, 1, emb_dim) 的张量 v_c.unsqueeze(1) 与形状为 (batch_size, emb_dim, K) 的张量 u_n.transpose(1, 2) 相乘；结果是(batch_size, 1, K)\n",
    "        negative_score = torch.bmm(v_c.unsqueeze(1), u_n.transpose(1, 2)).squeeze(1)# 最后的squeeze把维度为1的第二维去除，得到的结果维度是 (batch_size, K)\n",
    "        negative_score=torch.clamp(negative_score,min=-10,max=10)\n",
    "        negative_loss=-F.logsigmoid(-negative_score).sum(dim=1)\n",
    "        loss = postive_loss+negative_loss\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.emb_v.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_size))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.1814, 4.2008, 4.2172, 4.2307, 4.2414])\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "vacob_size =len(corpus.id2word)\n",
    "emb_size = 32\n",
    "model = SkipGram(vacob_size, emb_size)\n",
    "\n",
    "weight = torch.empty(vacob_size, emb_size)\n",
    "start_value = 0.01\n",
    "for i in range(vacob_size):\n",
    "    weight[i] = start_value + i * 0.01\n",
    "\n",
    "model.emb_v.weight.data.copy_(weight)\n",
    "model.emb_u.weight.data.copy_(weight)\n",
    "\n",
    "# Test the model\n",
    "center = torch.tensor([0, 1, 2, 3, 4])\n",
    "outside = torch.tensor([0, 1, 2, 3, 4])\n",
    "negative = torch.tensor([[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]])\n",
    "with torch.no_grad():\n",
    "    loss = model(center, outside, negative)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "# tensor([4.1814, 4.2008, 4.2172, 4.2307, 4.2414])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
