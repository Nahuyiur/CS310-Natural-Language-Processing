{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 9: Transformer\n",
    "\n",
    "In this lab, we will implement the multihead attention with future masking for causal language modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Implement Self-Attention for a Single Head\n",
    "\n",
    "First, prepare the input data in shape $N\\times d$\n",
    "\n",
    "*Hint*: \n",
    "- Use `torch.randn` to generate a torch tensor in the correct shape.\n",
    "\n",
    "在 Transformer 中，每个输入 token（比如一个词或子词）都会被编码成一个 d 维的向量（常见取值是 512 或 768）。而一个句子就是这些向量组成的矩阵，形状是 [N, d]，其中 N 是句子中 token 的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.size(): torch.Size([3, 512])\n",
      "X[:,0]: [-1.1258398  -0.54607284 -1.0840825 ]\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "d = 512\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "X = torch.randn(N, d)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test \n",
    "assert isinstance(X, torch.Tensor)\n",
    "print('X.size():', X.size())\n",
    "print('X[:,0]:', X[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# X.shape: (3, 512)\n",
    "# X[:,0]: [-1.1258398  -0.54607284 -1.0840825 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize weight matrices $W^Q$, $W^K$, and $W^V$. We assume they are for a single head, so $d_k=d_v=d$\n",
    "\n",
    "Using $W^Q$ as an example\n",
    "- First initialize an empty tensor `W_q` in the dimension of $d\\times d_k$, using the `torch.empty()` function. Then initialize it with `nn.init.xavier_uniform_()`.\n",
    "- After `W_q` is initialized, obtain the query matrix `Q` with a multiplication between `X` and `W_q`, using `torch.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.size(): torch.Size([3, 512])\n",
      "Q[:,0]: [-0.45352045 -0.40904033  0.18985942]\n",
      "K.size(): torch.Size([3, 512])\n",
      "K[:,0]: [ 1.509987   -0.5503683   0.44788954]\n",
      "V.size(): torch.Size([3, 512])\n",
      "V[:,0]: [ 0.43034226  0.00162293 -0.1317436 ]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # Do not remove this line\n",
    "\n",
    "n_heads = 1\n",
    "\n",
    "### START YOUR CODE ###\n",
    "d_k = d // n_heads # Compute d_k\n",
    "\n",
    "# 初始化权重矩阵 W_q, W_k, W_v\n",
    "W_q = torch.empty(d, d_k)\n",
    "W_k = torch.empty(d, d_k)\n",
    "W_v = torch.empty(d, d_k)\n",
    "\n",
    "# 使用 Xavier 初始化\n",
    "nn.init.xavier_normal_(W_q)\n",
    "nn.init.xavier_normal_(W_k)\n",
    "nn.init.xavier_normal_(W_v)\n",
    "\n",
    "# 计算 Q, K, V\n",
    "Q = torch.matmul(X, W_q)  # [N, d_k]\n",
    "K = torch.matmul(X, W_k)  # [N, d_k]\n",
    "V = torch.matmul(X, W_v)  # [N, d_k]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert Q.size() == (N, d_k)\n",
    "assert K.size() == (N, d_k)\n",
    "assert V.size() == (N, d_k)\n",
    "\n",
    "print('Q.size():', Q.size())\n",
    "print('Q[:,0]:', Q[:,0].data.numpy())\n",
    "print('K.size():', K.size())\n",
    "print('K[:,0]:', K[:,0].data.numpy())\n",
    "print('V.size():', V.size())\n",
    "print('V[:,0]:', V[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# Q.size(): torch.Size([3, 512])\n",
    "# Q[:,0]: [-0.45352045 -0.40904033  0.18985942]\n",
    "# K.size(): torch.Size([3, 512])\n",
    "# K[:,0]: [ 1.509987   -0.5503683   0.44788954]\n",
    "# V.size(): torch.Size([3, 512])\n",
    "# V[:,0]: [ 0.43034226  0.00162293 -0.1317436 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, compute the attention scores $\\alpha$ and the weighted output\n",
    "\n",
    "Following the equation:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "*Hint*:\n",
    "- $\\alpha = \\text{softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})$, where you can use `torch.nn.functional.softmax()` to compute the softmax. Pay attention to the `dim` parameter.\n",
    "- The weighted output is the multiplication between $\\alpha$ and $V$. Pay attention to their dimensions: $\\alpha$ is of shape $N\\times N$, and $\\alpha_{ij}$ is the attention score from the $i$-th to the $j$-th word. \n",
    "- The weighted output is of shape $N\\times d_v$, and here we assume $d_k=d_v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha.size(): torch.Size([3, 3])\n",
      "alpha: [[0.78344566 0.14102352 0.07553086]\n",
      " [0.25583813 0.18030964 0.5638523 ]\n",
      " [0.09271843 0.2767209  0.63056064]]\n",
      "output.size(): torch.Size([3, 512])\n",
      "output[:,0]: [ 0.32742795  0.03610666 -0.04272257]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# Step 1: QK^T / sqrt(d_k)\n",
    "scores = torch.matmul(Q, K.T) / (d_k ** 0.5)  # shape [N, N]\n",
    "\n",
    "# Step 2: softmax to get alpha\n",
    "alpha = F.softmax(scores, dim=1)  # shape [N, N]\n",
    "\n",
    "# Step 3: weighted sum of V\n",
    "output = torch.matmul(alpha, V)  # shape [N, d_v]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert alpha.size() == (N, N)\n",
    "assert output.size() == (N, d_k)\n",
    "\n",
    "print('alpha.size():', alpha.size())\n",
    "print('alpha:', alpha.data.numpy())\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# alpha.size(): torch.Size([3, 3])\n",
    "# alpha: [[0.78344566 0.14102352 0.07553086]\n",
    "#  [0.25583813 0.18030964 0.5638523 ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.32742795  0.03610666 -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Mask Future Tokens\n",
    "\n",
    "First, create a binary mask tensor of size $N\\times N$, which is lower triangular, with the diagonal and upper triangle set to 0.\n",
    "\n",
    "*Hint*: Use `torch.tril` and `torch.ones`.\n",
    "\n",
    "在实现 Transformer 的 decoder 部分时，我们需要一个 下三角掩码（mask），以防模型在训练过程中“偷看”未来的 token（实现 自回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: [[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "mask = torch.tril(torch.ones(N, N))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('mask:', mask.data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# mask: [[1. 0. 0.]\n",
    "#  [1. 1. 0.]\n",
    "#  [1. 1. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the mask to fill the corresponding future cells in $QK^\\top$ with $-\\infty$ (`-np.inf`), and then pass it to softmax to compute the new attention scores.\n",
    "\n",
    "*Hint*: Use `torch.Tensor.masked_fill` function to selectively fill the upper triangle area of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_alpha: [[1.         0.         0.        ]\n",
      " [0.5865858  0.41341412 0.        ]\n",
      " [0.09271843 0.2767209  0.63056064]]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "new_alpha = torch.nn.functional.softmax(\n",
    "    scores.masked_fill(mask == 0, float('-inf')), dim=1\n",
    ") # 将未来 token 的打分屏蔽为 -inf;对每一行做 softmax，即每个 token 对其它 token 的注意力分布\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print('new_alpha:', new_alpha.data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_alpha: [[1.         0.         0.        ]\n",
    "#  [0.5865858  0.41341412 0.        ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the output should also be updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_output.size(): torch.Size([3, 512])\n",
      "new_output[:,0]: [ 0.43034226  0.2531036  -0.04272257]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "new_output = torch.matmul(new_alpha, V)  # shape: [3, 512]\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('new_output.size():', new_output.size())\n",
    "print('new_output[:,0]:', new_output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_output.size(): torch.Size([3, 512])\n",
    "# new_output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T3. Integrate Multiple Heads\n",
    "\n",
    "Finally, integrate the above implemented functions into the `MultiHeadAttention` class.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- In this class, the weight matrices `W_q`, `W_k`, and `W_v` are defined as tensors of size $d\\times d$. Thus, the output $Q=XW^Q$ is of size $N\\times d$.\n",
    "\n",
    "- Then we reshape $Q$ (and $K$, $V$ as well) into the tensor `Q_` of shape $N\\times h\\times d_k$, where $h$ is the number of heads (`n_heads`) and $d_k = d // h$. Similar operations are applied to $K$ and $V$. \n",
    "\n",
    "- The multiplication $QK^\\top$ is now between two tensors of shape $N\\times h\\times d_k$, `Q_` and `K_`, and the output is of size $h\\times N \\times N$. Thus, you need to use `torch.matmul` and `torch.permute` properly to make the dimensions of `Q_`, `K_`, and `V_` be in the correct order.\n",
    "\n",
    "- Also, remember to apply the future mask to each attention head's output. You can use `torch.repeat` to replicate the mask for `n_heads` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_q = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_k = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_v = nn.Parameter(torch.empty(d_model, d_model))\n",
    "\n",
    "        nn.init.xavier_normal_(self.W_q)\n",
    "        nn.init.xavier_normal_(self.W_k)\n",
    "        nn.init.xavier_normal_(self.W_v)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        N = X.size(0)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # Step 1: QKV projection\n",
    "        Q = torch.matmul(X, self.W_q)  # [N, d_model]\n",
    "        K = torch.matmul(X, self.W_k)  # [N, d_model]\n",
    "        V = torch.matmul(X, self.W_v)  # [N, d_model]\n",
    "\n",
    "        # Step 2: reshape into multi-head format: [N, h, d_k]\n",
    "        Q_ = Q.view(N, self.n_heads, self.d_k)\n",
    "        K_ = K.view(N, self.n_heads, self.d_k)\n",
    "        V_ = V.view(N, self.n_heads, self.d_k)\n",
    "\n",
    "        # Step 3: permute to [h, N, d_k] for batched attention\n",
    "        Q_ = Q_.permute(1, 0, 2)  # [h, N, d_k]\n",
    "        K_ = K_.permute(1, 0, 2)\n",
    "        V_ = V_.permute(1, 0, 2)\n",
    "        \n",
    "        # Raw attention scores\n",
    "        scores = torch.matmul(Q_, K_.transpose(1, 2)) / (self.d_k ** 0.5)\n",
    "        # Apply the mask\n",
    "        mask = torch.tril(torch.ones(N, N, device=X.device)).bool()\n",
    "        mask = mask.unsqueeze(0).repeat(self.n_heads, 1, 1)\n",
    "        scores = scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        # Softmax\n",
    "        alpha = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum\n",
    "        context = torch.matmul(alpha, V_)\n",
    "\n",
    "        # Reshape back\n",
    "        context = context.permute(1, 0, 2).contiguous()\n",
    "        output = context.view(N, self.n_heads * self.d_k)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.size(): torch.Size([3, 512])\n",
      "output[:,0]: [ 0.43034226  0.2531036  -0.04272257]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "torch.manual_seed(0)\n",
    "\n",
    "multi_head_attn = MultiHeadAttention(d, n_heads=1)\n",
    "output = multi_head_attn(X)\n",
    "\n",
    "assert output.size() == (N, d)\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the above output size and values should be the same as the previous one, as we used `n_heads=1`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
