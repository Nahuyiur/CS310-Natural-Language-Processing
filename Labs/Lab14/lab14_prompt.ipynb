{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 14: In-Context Learning and Prompting\n",
    "\n",
    "In this lab, we will practice some in-context learning techniques, such as few-shot learning and chain-of-thought prompting, for solving QA problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1. Run LLMs locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Install llama.cpp\n",
    "\n",
    "Build the [llama.cpp](https://github.com/ggml-org/llama.cpp) tool, or download the binaries from the [release page](https://github.com/ggml-org/llama.cpp/releases).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1) Download model\n",
    "\n",
    "We are going to download the model that is quantized and format-converted to `gguf` format.\n",
    "\n",
    "**Model option a**: \n",
    "- Using the `huggingface-cli` tool.\n",
    "- Following the tutorial here: (Qwen2.5-7B-Instruct-GGUF)[https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-GGUF]\n",
    "\n",
    "A quick command to download the model is:\n",
    "```bash\n",
    "huggingface-cli download Qwen/Qwen1.5-7B-Chat-GGUF qwen1_5-7b-chat-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "\n",
    "**Model option b**: \n",
    "- Or you can download the ChatGLM-3 model from ModelScope: https://modelscope.cn/models/ZhipuAI/chatglm3-6b/files\n",
    "  - `model.safetensors.index.json`, `config.json`, `configuration.json`\n",
    "  - `model-00001-of-00007.safetensors` to `model-00007-of-00007.safetensors`\n",
    "  - `tokenizer_config.json`, `tokenizer.model`\n",
    "Put all the files in a folder such as `./chatglm3-6b`. \n",
    "- Then use tools like [`chatglm.cpp`](https://github.com/li-plus/chatglm.cpp) to manually convert the model weights to `ggml` format.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3) Run model\n",
    "\n",
    "You can run the model with following command:\n",
    "\n",
    "```bash\n",
    "llama-cli -m $MODEL_PATH\n",
    "```\n",
    "\n",
    "Then you can start interacting with the model in command line. Try to solve the following problems.\n",
    " - Use zero-shot and few-shot prompting to solve the problems.\n",
    " - Add Chain-of-Thought prompt if needed.\n",
    "\n",
    "\n",
    "Try solving these problems with prompting:\n",
    "1. Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there? A: \n",
    "2. 鸡和兔在一个笼子里，共有35个头，94只脚，那么鸡有多少只，兔有多少只？\n",
    "3. Q: 242342 + 423443 = ? A: \n",
    "4. 一个人花8块钱买了一只鸡，9块钱卖掉了，然后他觉得不划算，花10块钱又买回来了，11块卖给另外一个人。问他赚了多少?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers:\n",
    "## A1\n",
    "First, let's find out how many balls the juggler can juggle in total:\n",
    "\n",
    "16 balls\n",
    "\n",
    "Half of these balls are golf balls:\n",
    "\n",
    "\\( \\frac{16}{2} = 8 \\) golf balls\n",
    "\n",
    "Half of the golf balls are blue:\n",
    "\n",
    "\\( \\frac{8}{2} = 4 \\) blue golf balls\n",
    "\n",
    "So, there are 4 blue golf balls.\n",
    "## A2\n",
    "设鸡的数量为 \\( x \\)，兔的数量为 \\( y \\)。\n",
    "\n",
    "根据题意，我们有两个方程：\n",
    "1. 头的总数：\\( x + y = 35 \\) （因为每个头有1个)\n",
    "2. 脚的总数：鸡有2只脚，兔有4只脚，所以 \\( 2x + 4y = 94 \\) （因为总共有94只脚）\n",
    "\n",
    "首先解第一个方程得到 \\( y \\) 的表达式：\n",
    "\\( y = 35 - x \\)\n",
    "\n",
    "然后将 \\( y \\) 的表达式代入第二个方程：\n",
    "\\( 2x + 4(35 - x) = 94 \\)\n",
    "\n",
    "解这个方程：\n",
    "\\( 2x + 140 - 4x = 94 \\)\n",
    "\\( -2x = 94 - 140 \\)\n",
    "\\( -2x = -46 \\)\n",
    "\\( x = 23 \\)\n",
    "\n",
    "所以鸡有23只。\n",
    "\n",
    "现在用 \\( x \\) 的值来解 \\( y \\)：\n",
    "\\( y = 35 - x = 35 - 23 = 12 \\)\n",
    "\n",
    "兔子有12只。\n",
    "\n",
    "答案是：鸡有23只，兔有12只。\n",
    "## A3\n",
    "242342 + 423443 = 665785\n",
    "## A4\n",
    "这个人第一次交易赚了1块钱，因为他卖鸡得到9块钱，而买鸡花了8块钱。第二次交易他又赚了1块钱，因为他卖鸡得到11块钱，而买回来的成本是10块钱。\n",
    "\n",
    "所以，总共赚的钱是 \\(1 + 1 = 2\\) 元。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2. Practice few-shot prompting\n",
    "\n",
    "For this pratice, you need to first download the [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) model from HuggingFace, by running the following command:\n",
    "\n",
    "```bash\n",
    "huggingface-cli download Qwen/Qwen2.5-7B --local-dir $MODEL_PATH\n",
    "```\n",
    "\n",
    "The task set we use is [MMLU](https://huggingface.co/datasets/cais/mmlu). You need to download the zip file and extract it to the `./MMLU` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define some helper functions for constructing prompts and running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(input_list):\n",
    "    prompt = input_list[0]\n",
    "    k = len(input_list) - 2\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], input_list[j+1])\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "def format_shots(prompt_data):\n",
    "    prompt = \"\"\n",
    "    for data in prompt_data:\n",
    "        prompt += data[0]\n",
    "        k = len(data) - 2\n",
    "        for j in range(k):\n",
    "            prompt += \"\\n{}. {}\".format(choices[j], data[j+1])\n",
    "        prompt += \"\\nAnswer:\"\n",
    "        prompt += data[k+1] + \"\\n\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(input_list, subject, prompt_data):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject)\n",
    "    )\n",
    "    prompt += format_shots(prompt_data)\n",
    "    prompt += format_example(input_list)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `inference()` function constructs the full input by prepending the few-shot examples to the `input_text`, and generate **1** token as the output, because the task modality is multiple choice question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(tokenizer, model, input_text, subject, prompt_data):\n",
    "    if len(prompt_data) > 0:\n",
    "        full_input = gen_prompt(input_text, subject, prompt_data) # add few-shot examples\n",
    "    else:\n",
    "        full_input = input_text\n",
    "    inputs = tokenizer(full_input, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    ids = inputs['input_ids']\n",
    "    outputs = model.generate(\n",
    "                ids,\n",
    "                attention_mask = inputs['attention_mask'],\n",
    "                pad_token_id = tokenizer.eos_token_id,\n",
    "                max_new_tokens = 1, # Generate one token because it is multiple choice question\n",
    "                output_scores = True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "    logits = outputs['scores'][0][0]    #The first token\n",
    "    probs = (\n",
    "            torch.nn.functional.softmax(\n",
    "                torch.tensor(\n",
    "                    [\n",
    "                        logits[tokenizer(\"A\").input_ids[0]],\n",
    "                        logits[tokenizer(\"B\").input_ids[0]],\n",
    "                        logits[tokenizer(\"C\").input_ids[0]],\n",
    "                        logits[tokenizer(\"D\").input_ids[0]],\n",
    "                    ]\n",
    "                ),\n",
    "                dim=0,\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "    )\n",
    "    output_text = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}[np.argmax(probs)]\n",
    "    conf = np.max(probs)\n",
    "        \n",
    "    return output_text, full_input, conf.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "model_path = '/Volumes/Star/Qwen2.5-7B/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                          use_fast=True,\n",
    "                                          unk_token=\"<unk>\",\n",
    "                                          bos_token=\"<s>\", eos_token=\"</s>\",\n",
    "                                          add_bos_token=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "prompt = {}\n",
    "\n",
    "with open(f\"./MMLU/MMLU_ID_test.json\",'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "with open(f\"./MMLU/MMLU_ID_prompt.json\",'r') as f:\n",
    "    prompt = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data is organized by subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics'])\n",
      "\n",
      "['At breakfast, lunch, and dinner, Joe randomly chooses with equal '\n",
      " 'probabilities either an apple, an orange, or a banana to eat. On a given '\n",
      " 'day, what is the probability that Joe will eat at least two different kinds '\n",
      " 'of fruit?',\n",
      " '\\\\frac{7}{9}',\n",
      " '\\\\frac{8}{9}',\n",
      " '\\\\frac{5}{9}',\n",
      " '\\\\frac{9}{11}',\n",
      " 'B']\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())\n",
    "\n",
    "print()\n",
    "pprint(data['high_school_mathematics'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompts also come in subjects, and each subject has a list of 5 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(prompt['high_school_mathematics']))\n",
    "print(len(prompt['high_school_physics']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stick to one subject, `high_school_mathematics` for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'high_school_mathematics'\n",
    "data_sub = data[subject]\n",
    "prompt_sub = prompt[subject]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take one input example and generate the full prompt by calling `gen_prompt()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice questions (with answers) about  high school mathematics.\n",
      "\n",
      "Joe was in charge of lights for a dance. The red light blinks every two seconds, the yellow light every three seconds, and the blue light every five seconds. If we include the very beginning and very end of the dance, how many times during a seven minute dance will all the lights come on at the same time? (Assume that all three lights blink simultaneously at the very beginning of the dance.)\n",
      "A. 3\n",
      "B. 15\n",
      "C. 6\n",
      "D. 5\n",
      "Answer:B\n",
      "\n",
      "Five thousand dollars compounded annually at an $x\\%$ interest rate takes six years to double. At the same interest rate, how many years will it take $\\$300$ to grow to $\\$9600$?\n",
      "A. 12\n",
      "B. 1\n",
      "C. 30\n",
      "D. 5\n",
      "Answer:C\n",
      "\n",
      "The variable $x$ varies directly as the square of $y$, and $y$ varies directly as the cube of $z$. If $x$ equals $-16$ when $z$ equals 2, what is the value of $x$ when $z$ equals $\\frac{1}{2}$?\n",
      "A. -1\n",
      "B. 16\n",
      "C. -\\frac{1}{256}\n",
      "D. \\frac{1}{16}\n",
      "Answer:C\n",
      "\n",
      "Simplify and write the result with a rational denominator: $$\\sqrt{\\sqrt[3]{\\sqrt{\\frac{1}{729}}}}$$\n",
      "A. \\frac{3\\sqrt{3}}{3}\n",
      "B. \\frac{1}{3}\n",
      "C. \\sqrt{3}\n",
      "D. \\frac{\\sqrt{3}}{3}\n",
      "Answer:D\n",
      "\n",
      "Ten students take a biology test and receive the following scores: 45, 55, 50, 70, 65, 80, 40, 90, 70, 85. What is the mean of the students’ test scores?\n",
      "A. 55\n",
      "B. 60\n",
      "C. 62\n",
      "D. 65\n",
      "Answer:D\n",
      "\n",
      "At breakfast, lunch, and dinner, Joe randomly chooses with equal probabilities either an apple, an orange, or a banana to eat. On a given day, what is the probability that Joe will eat at least two different kinds of fruit?\n",
      "A. \\frac{7}{9}\n",
      "B. \\frac{8}{9}\n",
      "C. \\frac{5}{9}\n",
      "D. \\frac{9}{11}\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "input_text = data_sub[3]\n",
    "prompt_text = gen_prompt(input_text, subject, prompt_sub)\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.11/site-packages/transformers/generation/utils.py:2105: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on mps. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('mps') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output, _, conf = inference(tokenizer, model, input_text, subject, prompt_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "0.4174620807170868\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with zero-shot prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_prompt = '''\n",
    "    At breakfast, lunch, and dinner, Joe randomly chooses with equal probabilities either an apple, an orange, or a banana to eat. On a given day, what is the probability that Joe will eat at least two different kinds of fruit?\n",
    "    A. \\frac{7}{9}\n",
    "    B. \\frac{8}{9}\n",
    "    C. \\frac{5}{9}\n",
    "    D. \\frac{9}{11}\n",
    "    Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "0.40220537781715393\n"
     ]
    }
   ],
   "source": [
    "output, _, conf = inference(tokenizer, model, zs_prompt, subject, prompt_data=[])\n",
    "print(output)\n",
    "print(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
