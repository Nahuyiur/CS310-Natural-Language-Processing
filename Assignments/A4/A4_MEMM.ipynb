{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4 MEMM implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from utils import read_ner_data_from_connl, load_embedding_dict\n",
    "from utils import get_tag_indices_from_scores\n",
    "from utils import Indexer\n",
    "from metrics import MetricsHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_ner_data_from_connl('data/train.txt')\n",
    "dev_words, dev_tags = read_ner_data_from_connl('data/dev.txt')\n",
    "test_words, test_tags = read_ner_data_from_connl('data/test.txt')\n",
    "\n",
    "train_words = [word.lower() for word in train_words]\n",
    "dev_words = [word.lower() for word in dev_words]\n",
    "test_words = [word.lower() for word in test_words]\n",
    "\n",
    "word_indexer = Indexer(train_words)\n",
    "tag_indexer = Indexer(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NERDataset:\n",
    "    def __init__(self, words, tags, word_indexer, tag_indexer):\n",
    "        self.words = words\n",
    "        self.tags = tags\n",
    "        self.word_indexer = word_indexer\n",
    "        self.tag_indexer = tag_indexer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word_idx = self.word_indexer.element_to_index(self.words[idx])\n",
    "        tag_idx = self.tag_indexer.element_to_index(self.tags[idx])\n",
    "        return word_idx, tag_idx\n",
    "\n",
    "train_dataset = NERDataset(train_words, train_tags, word_indexer, tag_indexer)\n",
    "dev_dataset = NERDataset(dev_words, dev_tags, word_indexer, tag_indexer)\n",
    "test_dataset = NERDataset(test_words, test_tags, word_indexer, tag_indexer)\n",
    "\n",
    "def get_batch(dataset, batch_size):\n",
    "    words = dataset.words\n",
    "    tags = dataset.tags\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_words = words[i:i + batch_size]\n",
    "        batch_tags = tags[i:i + batch_size]\n",
    "        word_indices = [dataset.word_indexer.element_to_index(w) for w in batch_words]\n",
    "        tag_indices = [dataset.tag_indexer.element_to_index(t) for t in batch_tags]\n",
    "        yield (torch.tensor(word_indices, dtype=torch.long),\n",
    "               torch.tensor(tag_indices, dtype=torch.long))\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = list(get_batch(train_dataset, batch_size))\n",
    "dev_loader = list(get_batch(dev_dataset, batch_size))\n",
    "test_loader = list(get_batch(test_dataset, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:02<00:00, 139510.48it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = load_embedding_dict(glove_path)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_indexer), embedding_dim))\n",
    "for word, idx in word_indexer.get_element_to_index_dict().items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MEMMNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, tagset_size, tag_embedding_dim, embedding_matrix):\n",
    "        super(MEMMNER, self).__init__()\n",
    "\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), freeze=False)  \n",
    "\n",
    "        self.tag_embedding = nn.Embedding(tagset_size, tag_embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(embedding_dim + tag_embedding_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, words, prev_tags):\n",
    "    \n",
    "        word_emb = self.word_embedding(words)  # (batch_size, embedding_dim)\n",
    "        tag_emb = self.tag_embedding(prev_tags)  # (batch_size, tag_embedding_dim)\n",
    "\n",
    "        combined = torch.cat((word_emb, tag_emb), dim=-1)  # (batch_size, embedding_dim + tag_embedding_dim)\n",
    "        tag_scores = self.fc(combined)  # (batch_size, tagset_size)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_indexer)\n",
    "tagset_size = len(tag_indexer)\n",
    "tag_embedding_dim = 50  # 标签嵌入维度，可调整\n",
    "model = MEMMNER(vocab_size, embedding_dim, tagset_size, tag_embedding_dim, embedding_matrix)\n",
    "\n",
    "\n",
    "labels_str = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "labels_int = list(range(len(labels_str)))\n",
    "train_metrics = MetricsHandler(labels_int)\n",
    "dev_metrics = MetricsHandler(labels_int)\n",
    "test_metrics = MetricsHandler(labels_int)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, dev_loader, optimizer, loss_func, train_metrics, dev_metrics, num_epochs=5, device=None, train_dataset=None, dev_dataset=None):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    dev_f1_scores = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        train_metrics = MetricsHandler(labels_int)  \n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            words, tags = batch\n",
    "            words = words.to(device)\n",
    "            tags = tags.to(device)\n",
    "            \n",
    "            prev_tags = torch.zeros(words.size(0), dtype=torch.long).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(words, prev_tags)  # (batch_size, tagset_size)\n",
    "            loss = loss_func(output, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=-1).cpu().numpy()\n",
    "            train_metrics.update(predictions, tags.cpu().numpy())\n",
    "            running_loss += loss.item() * words.size(0)\n",
    "            \n",
    "            prev_tags = torch.argmax(output, dim=-1).detach()\n",
    "        \n",
    "        train_metrics.collect()\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        losses.append(epoch_loss)\n",
    "        train_f1 = train_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        \n",
    "        dev_loss, dev_metrics = evaluate_model(model, dev_loader, loss_func, dev_metrics, device, dev_dataset)\n",
    "        dev_metrics.collect()\n",
    "        dev_f1 = dev_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        dev_f1_scores.append(dev_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Train F1: {train_f1:.4f}, Dev F1: {dev_f1:.4f}\")\n",
    "    \n",
    "    return model, train_metrics, dev_metrics, losses, dev_f1_scores\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, loss_func, eval_metrics, device=None, dataset=None):\n",
    "    model.eval()\n",
    "    eval_metrics = MetricsHandler(labels_int)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            words, tags = batch\n",
    "            words = words.to(device)\n",
    "            tags = tags.to(device)\n",
    "            \n",
    "\n",
    "            prev_tags = torch.zeros(words.size(0), dtype=torch.long).to(device)\n",
    "            output = model(words, prev_tags)\n",
    "            loss = loss_func(output, tags)\n",
    "            total_loss += loss.item() * words.size(0)\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=-1).cpu().numpy()\n",
    "            eval_metrics.update(predictions, tags.cpu().numpy())\n",
    "    \n",
    "    eval_loss = total_loss / len(dataset)\n",
    "    return eval_loss, eval_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruiyuhan/Library/CloudStorage/OneDrive-Personal/自然语言处理/A4/metrics.py:9: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return x[1, 1] / (x[1, 0] + x[1, 1])\n",
      "/Users/ruiyuhan/Library/CloudStorage/OneDrive-Personal/自然语言处理/A4/metrics.py:5: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return x[1, 1]/(x[1, 1] + x[0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.3933, Train F1: 0.5730, Dev F1: 0.6869\n",
      "Epoch 2/10, Loss: 0.2106, Train F1: 0.7796, Dev F1: 0.7301\n",
      "Epoch 3/10, Loss: 0.1628, Train F1: 0.8275, Dev F1: 0.7390\n",
      "Epoch 4/10, Loss: 0.1399, Train F1: 0.8541, Dev F1: 0.7385\n",
      "Epoch 5/10, Loss: 0.1281, Train F1: 0.8615, Dev F1: 0.7371\n",
      "Epoch 6/10, Loss: 0.1217, Train F1: 0.8633, Dev F1: 0.7381\n",
      "Epoch 7/10, Loss: 0.1180, Train F1: 0.8635, Dev F1: 0.7379\n",
      "Epoch 8/10, Loss: 0.1158, Train F1: 0.8636, Dev F1: 0.7397\n",
      "Epoch 9/10, Loss: 0.1143, Train F1: 0.8635, Dev F1: 0.7395\n",
      "Epoch 10/10, Loss: 0.1134, Train F1: 0.8632, Dev F1: 0.7399\n",
      "模型参数已保存到 memm_ner_model.pth\n",
      "开始测试...\n",
      "Test F1 Score: 0.6391\n",
      "\n",
      "前5个周期的开发集F-1分数：\n",
      "Epoch 1: Dev F1 Score: 0.6869\n",
      "Epoch 2: Dev F1 Score: 0.7301\n",
      "Epoch 3: Dev F1 Score: 0.7390\n",
      "Epoch 4: Dev F1 Score: 0.7385\n",
      "Epoch 5: Dev F1 Score: 0.7371\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "print(\"开始训练...\")\n",
    "\n",
    "train_metrics = MetricsHandler(labels_int)\n",
    "dev_metrics = MetricsHandler(labels_int)\n",
    "\n",
    "model, train_metrics, dev_metrics, losses, dev_f1_scores = train_model(\n",
    "    model, train_loader, dev_loader, optimizer, loss_func, train_metrics, dev_metrics,\n",
    "    num_epochs=num_epochs, device=device, train_dataset=train_dataset, dev_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "\n",
    "model_path = \"memm_ner_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"模型参数已保存到 {model_path}\")\n",
    "\n",
    "\n",
    "print(\"开始测试...\")\n",
    "test_loss, test_metrics = evaluate_model(\n",
    "    model, test_loader, loss_func, test_metrics, device=device, dataset=test_dataset\n",
    ")\n",
    "test_metrics.collect()\n",
    "test_f1 = test_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n前5个周期的开发集F-1分数：\")\n",
    "for i, f1 in enumerate(dev_f1_scores[:5], 1):\n",
    "    print(f\"Epoch {i}: Dev F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
