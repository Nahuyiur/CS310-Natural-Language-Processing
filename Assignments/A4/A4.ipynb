{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from utils import Indexer, read_ner_data_from_connl, load_embedding_dict\n",
    "from metrics import MetricsHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 read_ner_data_from_connl 返回展平的单词和标签列表\n",
    "# 我们需要手动将它们按句子分组\n",
    "def group_into_sentences(words, tags):\n",
    "    \"\"\"\n",
    "    将展平的单词和标签列表按句子分组\n",
    "    假设空行（或特殊标记）分隔句子，但这里我们需要原始文件来准确分组\n",
    "    这里假设 words 和 tags 是按顺序对应且长度相等的\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    tag_sentences = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "    \n",
    "    # 简单假设：如果遇到特殊标记（例如 'DOCSTART' 或空行），则分隔句子\n",
    "    # 注意：实际 CoNLL 文件需要根据空行分隔，这里仅为示例\n",
    "    for word, tag in zip(words, tags):\n",
    "        if word == '-DOCSTART-':  # CoNLL 文件中的文档分隔符\n",
    "            if current_sentence:  # 如果当前句子不为空，保存\n",
    "                sentences.append(current_sentence)\n",
    "                tag_sentences.append(current_tags)\n",
    "                current_sentence = []\n",
    "                current_tags = []\n",
    "        else:\n",
    "            current_sentence.append(word.lower())  # 转换为小写\n",
    "            current_tags.append(tag)\n",
    "    \n",
    "    # 添加最后一个句子\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tag_sentences.append(current_tags)\n",
    "    \n",
    "    return sentences, tag_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "train_words, train_tags = read_ner_data_from_connl('data/train.txt')\n",
    "dev_words, dev_tags = read_ner_data_from_connl('data/dev.txt')\n",
    "test_words, test_tags = read_ner_data_from_connl('data/test.txt')\n",
    "\n",
    "# 按句子分组\n",
    "train_sentences, train_tag_sentences = group_into_sentences(train_words, train_tags)\n",
    "dev_sentences, dev_tag_sentences = group_into_sentences(dev_words, dev_tags)\n",
    "test_sentences, test_tag_sentences = group_into_sentences(test_words, test_tags)\n",
    "\n",
    "# 创建词汇表（基于所有单词）\n",
    "all_train_words = [word for sentence in train_sentences for word in sentence]\n",
    "word_indexer = Indexer(all_train_words)\n",
    "tag_indexer = Indexer([tag for tags in train_tag_sentences for tag in tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集类（处理句子级别数据）\n",
    "class NERDataset:\n",
    "    def __init__(self, sentences, tag_sentences, word_indexer, tag_indexer):\n",
    "        self.sentences = sentences  # 句子列表\n",
    "        self.tag_sentences = tag_sentences  # 标签序列列表\n",
    "        self.word_indexer = word_indexer\n",
    "        self.tag_indexer = tag_indexer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = self.sentences[idx]\n",
    "        tags = self.tag_sentences[idx]\n",
    "        word_indices = [self.word_indexer.element_to_index(word) for word in words]\n",
    "        tag_indices = [self.tag_indexer.element_to_index(tag) for tag in tags]\n",
    "        mask = [1] * len(words)  # 掩码，表示有效位置\n",
    "        return word_indices, tag_indices, mask\n",
    "\n",
    "# 数据加载器（返回批次化的句子数据）\n",
    "def get_batch(dataset, batch_size):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_samples = [dataset[j] for j in range(i, min(i + batch_size, len(dataset)))]\n",
    "        batch_words, batch_tags, batch_masks = zip(*batch_samples)\n",
    "        \n",
    "        # 填充到最大长度\n",
    "        max_len = max(len(seq) for seq in batch_words)\n",
    "        word_indices = torch.tensor([seq + [0] * (max_len - len(seq)) for seq in batch_words], dtype=torch.long)\n",
    "        tag_indices = torch.tensor([seq + [0] * (max_len - len(seq)) for seq in batch_tags], dtype=torch.long)\n",
    "        masks = torch.tensor([m + [0] * (max_len - len(m)) for m in batch_masks], dtype=torch.float)\n",
    "        \n",
    "        yield word_indices, tag_indices, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:02<00:00, 149431.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集和加载器\n",
    "batch_size = 128\n",
    "train_dataset = NERDataset(train_sentences, train_tag_sentences, word_indexer, tag_indexer)\n",
    "dev_dataset = NERDataset(dev_sentences, dev_tag_sentences, word_indexer, tag_indexer)\n",
    "test_dataset = NERDataset(test_sentences, test_tag_sentences, word_indexer, tag_indexer)\n",
    "\n",
    "train_loader = list(get_batch(train_dataset, batch_size))\n",
    "dev_loader = list(get_batch(dev_dataset, batch_size))\n",
    "test_loader = list(get_batch(test_dataset, batch_size))\n",
    "\n",
    "# 加载 GloVe 嵌入\n",
    "glove_path = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = load_embedding_dict(glove_path)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_indexer), embedding_dim))\n",
    "for word, idx in word_indexer.get_element_to_index_dict().items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags):\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        self.transition = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        # 可选：对填充标签的转移施加惩罚\n",
    "        # self.transition.data[:, 0] = -10000  # 禁止转移到填充标签\n",
    "        # self.transition.data[0, :] = -10000  # 禁止从填充标签转移出\n",
    "\n",
    "    def forward(self, emissions, tags, mask=None):\n",
    "        batch_size, seq_len, _ = emissions.shape\n",
    "\n",
    "        # 计算真实路径的分数\n",
    "        score = emissions[range(batch_size), 0, tags[:, 0]]  # 初始位置的发射分数\n",
    "        for t in range(1, seq_len):\n",
    "            score += self.transition[tags[:, t-1], tags[:, t]]  # 转移分数\n",
    "            score += emissions[range(batch_size), t, tags[:, t]]  # 后续位置的发射分数\n",
    "\n",
    "        # 计算所有路径的总分数（配分函数）\n",
    "        alpha = emissions[:, 0]  # (batch_size, num_tags)\n",
    "        for t in range(1, seq_len):\n",
    "            alpha_t = alpha.unsqueeze(1) + self.transition  # (batch_size, num_tags, num_tags)\n",
    "            alpha_t = torch.logsumexp(alpha_t, dim=2) + emissions[:, t]  # (batch_size, num_tags)\n",
    "            if mask is not None:\n",
    "                # 使用掩码处理填充位置\n",
    "                alpha_t = alpha_t * mask[:, t].unsqueeze(1) + alpha * (1 - mask[:, t]).unsqueeze(1)\n",
    "            alpha = alpha_t\n",
    "\n",
    "        total_score = torch.logsumexp(alpha, dim=1)  # (batch_size,)\n",
    "\n",
    "        # 返回负对数似然损失\n",
    "        return (total_score - score).mean()\n",
    "\n",
    "    def decode(self, emissions, mask=None):\n",
    "        batch_size, seq_len, num_tags = emissions.shape\n",
    "        viterbi = torch.zeros(batch_size, seq_len, num_tags, device=emissions.device)\n",
    "        backpointers = torch.zeros(batch_size, seq_len, num_tags, dtype=torch.long, device=emissions.device)\n",
    "\n",
    "        viterbi[:, 0, :] = emissions[:, 0, :]\n",
    "        for t in range(1, seq_len):\n",
    "            viterbi_t, indices = (viterbi[:, t-1, :].unsqueeze(2) + self.transition).max(dim=1)\n",
    "            viterbi[:, t, :] = viterbi_t + emissions[:, t, :]\n",
    "            backpointers[:, t, :] = indices\n",
    "\n",
    "        best_path = []\n",
    "        best_score, best_tag = viterbi[:, -1, :].max(dim=1)\n",
    "        best_path.append(best_tag)\n",
    "        for t in reversed(range(1, seq_len)):\n",
    "            best_tag = backpointers[:, t, best_tag]\n",
    "            best_path.append(best_tag)\n",
    "        best_path = torch.stack(best_path[::-1], dim=1)\n",
    "        return best_path\n",
    "\n",
    "# BiLSTM + CRF 模型\n",
    "class BiLSTM_CRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.crf = CRF(tagset_size)\n",
    "\n",
    "    def forward(self, x, tags=None, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        emissions = self.fc(lstm_out)\n",
    "        if tags is not None:\n",
    "            loss = self.crf(emissions, tags, mask)\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "vocab_size = len(word_indexer)\n",
    "tagset_size = len(tag_indexer)\n",
    "hidden_dim = 256\n",
    "model = BiLSTM_CRF_NER(vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_model(model, train_loader, dev_loader, optimizer, num_epochs=10, device=None):\n",
    "    model.train()\n",
    "\n",
    "    dev_f1_scores = []\n",
    "    labels_int = list(range(tagset_size))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        train_metrics = MetricsHandler(labels_int)\n",
    "        \n",
    "        batch_num=1\n",
    "\n",
    "        for batch in train_loader:\n",
    "            inputs, targets, masks = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(inputs, targets, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = model(inputs, mask=masks)\n",
    "            train_metrics.update(preds.cpu().numpy().flatten(), targets.cpu().numpy().flatten())\n",
    "\n",
    "            print(f\"Batch: {batch_num}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "            batch_num+=1\n",
    "        train_f1 = train_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        dev_metrics = evaluate_model(model, dev_loader, device)\n",
    "        dev_f1 = dev_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        dev_f1_scores.append(dev_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | Train F1: {train_f1:.4f} | Dev F1: {dev_f1:.4f}\")\n",
    "    \n",
    "    return model, dev_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 评估函数\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    labels_int = list(range(tagset_size))\n",
    "    metrics = MetricsHandler(labels_int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets, masks = [b.to(device) for b in batch]\n",
    "            preds = model(inputs, mask=masks)\n",
    "            metrics.update(preds.cpu().numpy().flatten(), targets.cpu().numpy().flatten())\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n"
     ]
    }
   ],
   "source": [
    "# 主程序部分\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "print(\"开始训练...\")\n",
    "\n",
    "model, dev_f1_scores = train_model(\n",
    "    model, train_loader, dev_loader, optimizer, num_epochs=num_epochs, device=device\n",
    ")\n",
    "\n",
    "model_path = \"bilstm_crf_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"模型参数已保存到 {model_path}\")\n",
    "\n",
    "print(\"\\n前5个周期的开发集F-1分数：\")\n",
    "for i, f1 in enumerate(dev_f1_scores[:5], 1):\n",
    "    print(f\"Epoch {i}: Dev F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\n开始测试...\")\n",
    "test_metrics = evaluate_model(model, test_loader, device)\n",
    "test_f1 = test_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "test_f1_greedy = 0.8245  # 请替换为实际值\n",
    "print(f\"\\n性能比较：\")\n",
    "print(f\"Test F1 Score (Greedy Search, Step 3): {test_f1_greedy:.4f}\")\n",
    "print(f\"Test F1 Score (CRF with Viterbi): {test_f1:.4f}\")\n",
    "print(f\"性能差异 (CRF - Greedy): {test_f1 - test_f1_greedy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
