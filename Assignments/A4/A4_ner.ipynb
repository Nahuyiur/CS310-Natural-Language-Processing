{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Long Short Term Memory (LSTM) Network for Named Entity Recognition (NER)\n",
    "\n",
    "**Total points**: 50 + (10 bonus)\n",
    "\n",
    "In this assignment, you will implement a Long Short Term Memory (LSTM) network for Named Entity Recognition (NER). \n",
    "\n",
    "Re-use the code in Lab 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import Indexer, read_ner_data_from_connl, load_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_ner_data_from_connl('data/train.txt')\n",
    "dev_words, dev_tags = read_ner_data_from_connl('data/dev.txt')\n",
    "test_words, test_tags = read_ner_data_from_connl('data/test.txt')\n",
    "\n",
    "train_words = [word.lower() for word in train_words]\n",
    "dev_words = [word.lower() for word in dev_words]\n",
    "test_words = [word.lower() for word in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    def __init__(self, elements):\n",
    "        self.element_to_idx = {e: i for i, e in enumerate(sorted(set(elements)))}\n",
    "    def element_to_index(self, element):\n",
    "        return self.element_to_idx.get(element, 0)  # 0 for unknown\n",
    "    def get_element_to_index_dict(self):\n",
    "        return self.element_to_idx\n",
    "    def __len__(self):\n",
    "        return len(self.element_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexer = Indexer(train_words)\n",
    "tag_indexer = Indexer(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset:\n",
    "    def __init__(self, words, tags, word_indexer, tag_indexer):\n",
    "        self.words = words\n",
    "        self.tags = tags\n",
    "        self.word_indexer = word_indexer\n",
    "        self.tag_indexer = tag_indexer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word_idx = self.word_indexer.element_to_index(self.words[idx])\n",
    "        tag_idx = self.tag_indexer.element_to_index(self.tags[idx])\n",
    "        return word_idx, tag_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_words, train_tags, word_indexer, tag_indexer)\n",
    "dev_dataset = NERDataset(dev_words, dev_tags, word_indexer, tag_indexer)\n",
    "test_dataset = NERDataset(test_words, test_tags, word_indexer, tag_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size):\n",
    "    words = dataset.words\n",
    "    tags = dataset.tags\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_words = words[i:i + batch_size]\n",
    "        batch_tags = tags[i:i + batch_size]\n",
    "        word_indices = [dataset.word_indexer.element_to_index(w) for w in batch_words]\n",
    "        tag_indices = [dataset.tag_indexer.element_to_index(t) for t in batch_tags]\n",
    "        yield (torch.tensor(word_indices, dtype=torch.long),\n",
    "               torch.tensor(tag_indices, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = list(get_batch(train_dataset, batch_size))\n",
    "dev_loader = list(get_batch(dev_dataset, batch_size))\n",
    "test_loader = list(get_batch(test_dataset, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:02<00:00, 157053.49it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = load_embedding_dict(glove_path)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_indexer), embedding_dim))\n",
    "for word, idx in word_indexer.get_element_to_index_dict().items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), freeze=False)  \n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, \n",
    "                           bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)，输入是单词索引\n",
    "        emb = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(emb)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        tag_scores = self.fc(lstm_out)  # (batch_size, seq_len, tagset_size)\n",
    "        return tag_scores\n",
    "    \n",
    "\n",
    "vocab_size = len(word_indexer)\n",
    "tagset_size = len(tag_indexer)\n",
    "hidden_dim = 256  \n",
    "model = BiLSTMNER(vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_tag_indices_from_scores\n",
    "from metrics import MetricsHandler\n",
    "import torch.optim as optim\n",
    "\n",
    "labels_str = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "labels_int = list(range(len(labels_str)))\n",
    "train_metrics = MetricsHandler(labels_int)\n",
    "dev_metrics = MetricsHandler(labels_int)\n",
    "test_metrics = MetricsHandler(labels_int)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 修改train_model函数，修复epoch_loss计算\n",
    "def train_model(model, train_loader, dev_loader, optimizer, loss_func, train_metrics, dev_metrics, num_epochs=5, device=None, train_dataset=None, dev_dataset=None, **kwargs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    dev_f1_scores = []  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        train_metrics = MetricsHandler(labels_int) \n",
    "        \n",
    "        # 训练循环\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)  # (batch_size, seq_len, tagset_size)\n",
    "\n",
    "            output = output.view(-1, tagset_size)\n",
    "            targets_flat = targets.view(-1)\n",
    "\n",
    "            loss = loss_func(output, targets_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predictions = get_tag_indices_from_scores(output.detach().cpu().numpy())\n",
    "            train_metrics.update(predictions, targets_flat.cpu().numpy())\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_metrics.collect()\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        losses.append(epoch_loss)\n",
    "        train_f1 = train_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        \n",
    "\n",
    "        dev_loss, dev_metrics = evaluate_model(model, dev_loader, loss_func, dev_metrics, device=device, dataset=dev_dataset)\n",
    "        dev_metrics.collect()\n",
    "        dev_f1 = dev_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        dev_f1_scores.append(dev_f1) \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Train F1: {train_f1:.4f}, Dev F1: {dev_f1:.4f}\")\n",
    "\n",
    "    return model, train_metrics, dev_metrics, losses, dev_f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_func, eval_metrics, device=None, dataset=None, **kwargs):\n",
    "\n",
    "    model.eval()\n",
    "    eval_metrics = MetricsHandler(labels_int)\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            output = output.view(-1, tagset_size)\n",
    "            targets_flat = targets.view(-1)\n",
    "\n",
    "            loss = loss_func(output, targets_flat)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            predictions = get_tag_indices_from_scores(output.cpu().numpy())\n",
    "            eval_metrics.update(predictions, targets_flat.cpu().numpy())\n",
    "\n",
    "\n",
    "    eval_loss = total_loss / len(dataset)\n",
    "    return eval_loss, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "Epoch 1/10, Loss: 0.0018, Train F1: 0.9981, Dev F1: 0.8794\n",
      "Epoch 2/10, Loss: 0.0019, Train F1: 0.9984, Dev F1: 0.8866\n",
      "Epoch 3/10, Loss: 0.0019, Train F1: 0.9982, Dev F1: 0.8800\n",
      "Epoch 4/10, Loss: 0.0012, Train F1: 0.9985, Dev F1: 0.8822\n",
      "Epoch 5/10, Loss: 0.0014, Train F1: 0.9987, Dev F1: 0.8822\n",
      "Epoch 6/10, Loss: 0.0016, Train F1: 0.9985, Dev F1: 0.8731\n",
      "Epoch 7/10, Loss: 0.0012, Train F1: 0.9988, Dev F1: 0.8667\n",
      "Epoch 8/10, Loss: 0.0014, Train F1: 0.9989, Dev F1: 0.8688\n",
      "Epoch 9/10, Loss: 0.0009, Train F1: 0.9993, Dev F1: 0.8828\n",
      "Epoch 10/10, Loss: 0.0005, Train F1: 0.9994, Dev F1: 0.8803\n",
      "模型参数已保存到 bilstm_ner_model.pth\n",
      "\n",
      "前5个周期的开发集F-1分数：\n",
      "Epoch 1: Dev F1 Score: 0.8794\n",
      "Epoch 2: Dev F1 Score: 0.8866\n",
      "Epoch 3: Dev F1 Score: 0.8800\n",
      "Epoch 4: Dev F1 Score: 0.8822\n",
      "Epoch 5: Dev F1 Score: 0.8822\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 10  \n",
    "print(\"开始训练...\")\n",
    "\n",
    "\n",
    "train_metrics = MetricsHandler(labels_int)\n",
    "dev_metrics = MetricsHandler(labels_int)\n",
    "\n",
    "\n",
    "model, train_metrics, dev_metrics, losses, dev_f1_scores = train_model(\n",
    "    model, train_loader, dev_loader, optimizer, loss_func, train_metrics, dev_metrics,\n",
    "    num_epochs=num_epochs, device=device, train_dataset=train_dataset, dev_dataset=dev_dataset\n",
    ")\n",
    "\n",
    "\n",
    "model_path = \"bilstm_ner_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"模型参数已保存到 {model_path}\")\n",
    "\n",
    "\n",
    "print(\"\\n前5个周期的开发集F-1分数：\")\n",
    "for i, f1 in enumerate(dev_f1_scores[:5], 1):\n",
    "    print(f\"Epoch {i}: Dev F1 Score: {f1:.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试...\n",
      "Test F1 Score: 0.8245\n"
     ]
    }
   ],
   "source": [
    "print(\"开始测试...\")\n",
    "test_loss, test_metrics = evaluate_model(\n",
    "    model, test_loader, loss_func, test_metrics, \n",
    "    device=device, dataset=test_dataset\n",
    ")\n",
    "test_metrics.collect()\n",
    "test_f1 = test_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
