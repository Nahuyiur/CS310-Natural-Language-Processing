{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Long Short Term Memory (LSTM) Network for Named Entity Recognition (NER)\n",
    "## CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import Indexer, read_ner_data_from_connl, load_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_ner_data_from_connl('data/train.txt')\n",
    "dev_words, dev_tags = read_ner_data_from_connl('data/dev.txt')\n",
    "test_words, test_tags = read_ner_data_from_connl('data/test.txt')\n",
    "\n",
    "train_words = [word.lower() for word in train_words]\n",
    "dev_words = [word.lower() for word in dev_words]\n",
    "test_words = [word.lower() for word in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexer = Indexer(train_words)\n",
    "tag_indexer = Indexer(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset:\n",
    "    def __init__(self, words, tags, word_indexer, tag_indexer):\n",
    "        self.words = words\n",
    "        self.tags = tags\n",
    "        self.word_indexer = word_indexer\n",
    "        self.tag_indexer = tag_indexer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word_idx = self.word_indexer.element_to_index(self.words[idx])\n",
    "        tag_idx = self.tag_indexer.element_to_index(self.tags[idx])\n",
    "        mask=1\n",
    "        return word_idx, tag_idx,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_words, train_tags, word_indexer, tag_indexer)\n",
    "dev_dataset = NERDataset(dev_words, dev_tags, word_indexer, tag_indexer)\n",
    "test_dataset = NERDataset(test_words, test_tags, word_indexer, tag_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整数据加载器以返回mask\n",
    "def get_batch(dataset, batch_size):\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_words = dataset.words[i:i + batch_size]\n",
    "        batch_tags = dataset.tags[i:i + batch_size]\n",
    "        word_indices = [dataset.word_indexer.element_to_index(w) for w in batch_words]\n",
    "        tag_indices = [dataset.tag_indexer.element_to_index(t) for t in batch_tags]\n",
    "        masks = [1] * len(batch_words)  # 简化处理，实际需根据填充符生成\n",
    "        yield (\n",
    "            torch.tensor(word_indices, dtype=torch.long),\n",
    "            torch.tensor(tag_indices, dtype=torch.long),\n",
    "            torch.tensor(masks, dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = list(get_batch(train_dataset, batch_size))\n",
    "dev_loader = list(get_batch(dev_dataset, batch_size))\n",
    "test_loader = list(get_batch(test_dataset, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:02<00:00, 150813.74it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = load_embedding_dict(glove_path)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_indexer), embedding_dim))\n",
    "for word, idx in word_indexer.get_element_to_index_dict().items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, num_tags):\n",
    "        super().__init__()\n",
    "        self.num_tags = num_tags\n",
    "        # 转移矩阵：transition[i][j]表示从标签i转移到j的分数\n",
    "        self.transition = nn.Parameter(torch.randn(num_tags, num_tags))\n",
    "        self.transition.data[:, 0] = -10000  # 禁止转移到起始标签（假设标签0为填充符）\n",
    "        self.transition.data[0, :] = -10000  # 禁止从起始标签转移出\n",
    "\n",
    "    def forward(self, emissions, tags, mask=None):\n",
    "        # emissions: (batch_size, seq_len, num_tags) 来自LSTM的输出\n",
    "        # tags: (batch_size, seq_len) 真实标签\n",
    "        # mask: (batch_size, seq_len) 表示有效位置（非填充）\n",
    "        batch_size, seq_len, _ = emissions.shape\n",
    "        \n",
    "        # 计算正确路径的分数\n",
    "        score = torch.gather(emissions, 2, tags.unsqueeze(-1)).squeeze(-1)  # 发射分数\n",
    "        score += self.transition[tags[:, :-1], tags[:, 1:]]  # 转移分数\n",
    "        \n",
    "        # 计算所有路径的总分数（配分函数）\n",
    "        alpha = emissions[:, 0]  # 初始状态\n",
    "        for t in range(1, seq_len):\n",
    "            alpha_t = alpha.unsqueeze(2) + self.transition.unsqueeze(0)  # (batch_size, num_tags, num_tags)\n",
    "            alpha_t = torch.logsumexp(alpha_t, dim=1) + emissions[:, t]\n",
    "            alpha = alpha_t * mask[:, t].unsqueeze(-1) + alpha * (1 - mask[:, t]).unsqueeze(-1)\n",
    "        \n",
    "        total_score = torch.logsumexp(alpha, dim=1)\n",
    "        return (total_score - score.sum(dim=1)).mean()  # 负对数似然损失\n",
    "\n",
    "    def decode(self, emissions, mask=None):\n",
    "        # 使用Viterbi算法解码最优路径\n",
    "        batch_size, seq_len, _ = emissions.shape\n",
    "        viterbi = torch.zeros_like(emissions)\n",
    "        backpointers = torch.zeros((batch_size, seq_len, self.num_tags), dtype=torch.long)\n",
    "        \n",
    "        # 初始化\n",
    "        viterbi[:, 0] = emissions[:, 0]\n",
    "        for t in range(1, seq_len):\n",
    "            max_scores, indices = (viterbi[:, t-1].unsqueeze(-1) + self.transition).max(dim=1)\n",
    "            viterbi[:, t] = max_scores + emissions[:, t]\n",
    "            backpointers[:, t] = indices\n",
    "        \n",
    "        # 回溯最优路径\n",
    "        best_path = []\n",
    "        best_score, best_tag = viterbi[:, -1].max(dim=1)\n",
    "        best_path.append(best_tag)\n",
    "        for t in reversed(range(seq_len-1)):\n",
    "            best_tag = backpointers[:, t+1].gather(1, best_tag.unsqueeze(-1)).squeeze(-1)\n",
    "            best_path.append(best_tag)\n",
    "        best_path = torch.stack(best_path[::-1], dim=1)\n",
    "        return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF_NER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        self.crf = CRF(tagset_size)\n",
    "\n",
    "    def forward(self, x, tags=None, mask=None):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # (batch_size, seq_len, hidden_dim*2)\n",
    "        emissions = self.fc(lstm_out)  # (batch_size, seq_len, tagset_size)\n",
    "        \n",
    "        if tags is not None:\n",
    "            loss = self.crf(emissions, tags, mask)\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask)\n",
    "\n",
    "# 初始化模型\n",
    "vocab_size = len(word_indexer)\n",
    "tagset_size = len(tag_indexer)\n",
    "hidden_dim = 256\n",
    "model = BiLSTM_CRF_NER(vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_tag_indices_from_scores\n",
    "from metrics import MetricsHandler\n",
    "import torch.optim as optim\n",
    "\n",
    "labels_str = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "labels_int = list(range(len(labels_str)))\n",
    "train_metrics = MetricsHandler(labels_int)\n",
    "dev_metrics = MetricsHandler(labels_int)\n",
    "test_metrics = MetricsHandler(labels_int)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_model(model, train_loader, dev_loader, optimizer, num_epochs=10, device=None):\n",
    "    \"\"\"\n",
    "    训练模型并在每个 epoch 后评估开发集\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    dev_f1_scores = []  # 存储每个 epoch 的开发集 F1 分数\n",
    "    labels_int = list(range(tagset_size))  # 标签的整数表示\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        train_metrics = MetricsHandler(labels_int)  # 训练集指标\n",
    "        \n",
    "        batch_num=1\n",
    "        for batch in train_loader:\n",
    "            inputs, targets, masks = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(inputs, targets, masks)  # 计算 CRF 损失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 解码预测结果并更新指标\n",
    "            preds = model(inputs, mask=masks)\n",
    "            train_metrics.update(preds.cpu().numpy().flatten(), targets.cpu().numpy().flatten())\n",
    "            print(f\"Batch: {batch_num}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "            batch_num+=1\n",
    "        \n",
    "        # 计算训练集 F1 分数\n",
    "        train_f1 = train_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        \n",
    "        # 评估开发集\n",
    "        dev_metrics = evaluate_model(model, dev_loader, device)\n",
    "        dev_f1 = dev_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        dev_f1_scores.append(dev_f1)\n",
    "        \n",
    "        # 打印当前 epoch 的结果\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | Train F1: {train_f1:.4f} | Dev F1: {dev_f1:.4f}\")\n",
    "    \n",
    "    return model, dev_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估函数\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    评估模型并返回指标\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    labels_int = list(range(tagset_size))\n",
    "    metrics = MetricsHandler(labels_int)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets, masks = [b.to(device) for b in batch]\n",
    "            preds = model(inputs, mask=masks)  # 使用维特比解码\n",
    "            metrics.update(preds.cpu().numpy().flatten(), targets.cpu().numpy().flatten())\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始训练...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 训练模型并获取开发集 F-1 分数\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model, dev_f1_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 保存模型参数\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilstm_crf_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[60], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, dev_loader, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs, targets, masks \u001b[38;5;241m=\u001b[39m [b\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 计算 CRF 损失\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[58], line 15\u001b[0m, in \u001b[0;36mBiLSTM_CRF_NER.forward\u001b[0;34m(self, x, tags, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m emissions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(lstm_out)  \u001b[38;5;66;03m# (batch_size, seq_len, tagset_size)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43memissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/NLP/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[57], line 14\u001b[0m, in \u001b[0;36mCRF.forward\u001b[0;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, emissions, tags, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# emissions: (batch_size, seq_len, num_tags) 来自LSTM的输出\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# tags: (batch_size, seq_len) 真实标签\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# mask: (batch_size, seq_len) 表示有效位置（非填充）\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     batch_size, seq_len, _ \u001b[38;5;241m=\u001b[39m emissions\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# 计算正确路径的分数\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(emissions, \u001b[38;5;241m2\u001b[39m, tags\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 发射分数\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# 主程序部分\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "print(\"开始训练...\")\n",
    "\n",
    "# 训练模型并获取开发集 F-1 分数\n",
    "model, dev_f1_scores = train_model(\n",
    "    model, train_loader, dev_loader, optimizer, num_epochs=num_epochs, device=device\n",
    ")\n",
    "\n",
    "# 保存模型参数\n",
    "model_path = \"bilstm_crf_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"模型参数已保存到 {model_path}\")\n",
    "\n",
    "# 打印前 5 个周期的开发集 F-1 分数\n",
    "print(\"\\n前5个周期的开发集F-1分数：\")\n",
    "for i, f1 in enumerate(dev_f1_scores[:5], 1):\n",
    "    print(f\"Epoch {i}: Dev F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 在测试集上评估\n",
    "print(\"\\n开始测试...\")\n",
    "test_metrics = evaluate_model(model, test_loader, device)\n",
    "test_f1 = test_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# 与第 3 步的贪心搜索结果比较（假设第 3 步的 F1 分数已知）\n",
    "test_f1_greedy = 0.8245  # 请替换为你在第 3 步中实际得到的 F1 分数\n",
    "print(f\"\\n性能比较：\")\n",
    "print(f\"Test F1 Score (Greedy Search, Step 3): {test_f1_greedy:.4f}\")\n",
    "print(f\"Test F1 Score (CRF with Viterbi): {test_f1:.4f}\")\n",
    "print(f\"性能差异 (CRF - Greedy): {test_f1 - test_f1_greedy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
