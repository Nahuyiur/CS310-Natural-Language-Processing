{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Long Short Term Memory (LSTM) Network for Named Entity Recognition (NER)\n",
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import Indexer, read_ner_data_from_connl, load_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_ner_data_from_connl('data/train.txt')\n",
    "dev_words, dev_tags = read_ner_data_from_connl('data/dev.txt')\n",
    "test_words, test_tags = read_ner_data_from_connl('data/test.txt')\n",
    "\n",
    "train_words = [word.lower() for word in train_words]\n",
    "dev_words = [word.lower() for word in dev_words]\n",
    "test_words = [word.lower() for word in test_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexer = Indexer(train_words)\n",
    "tag_indexer = Indexer(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset:\n",
    "    def __init__(self, words, tags, word_indexer, tag_indexer):\n",
    "        self.words = words\n",
    "        self.tags = tags\n",
    "        self.word_indexer = word_indexer\n",
    "        self.tag_indexer = tag_indexer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word_idx = self.word_indexer.element_to_index(self.words[idx])\n",
    "        tag_idx = self.tag_indexer.element_to_index(self.tags[idx])\n",
    "        return word_idx, tag_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(train_words, train_tags, word_indexer, tag_indexer)\n",
    "dev_dataset = NERDataset(dev_words, dev_tags, word_indexer, tag_indexer)\n",
    "test_dataset = NERDataset(test_words, test_tags, word_indexer, tag_indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, batch_size):\n",
    "    words = dataset.words\n",
    "    tags = dataset.tags\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_words = words[i:i + batch_size]\n",
    "        batch_tags = tags[i:i + batch_size]\n",
    "        word_indices = [dataset.word_indexer.element_to_index(w) for w in batch_words]\n",
    "        tag_indices = [dataset.tag_indexer.element_to_index(t) for t in batch_tags]\n",
    "        yield (torch.tensor(word_indices, dtype=torch.long),\n",
    "               torch.tensor(tag_indices, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = list(get_batch(train_dataset, batch_size))\n",
    "dev_loader = list(get_batch(dev_dataset, batch_size))\n",
    "test_loader = list(get_batch(test_dataset, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:02<00:00, 140465.51it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = load_embedding_dict(glove_path)\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_indexer), embedding_dim))\n",
    "for word, idx in word_indexer.get_element_to_index_dict().items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix):\n",
    "        super(BiLSTMNER, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix), freeze=False)  \n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, \n",
    "                           bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)，输入是单词索引\n",
    "        emb = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(emb)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "        tag_scores = self.fc(lstm_out)  # (batch_size, seq_len, tagset_size)\n",
    "        return tag_scores\n",
    "    \n",
    "\n",
    "vocab_size = len(word_indexer)\n",
    "tagset_size = len(tag_indexer)\n",
    "hidden_dim = 256  \n",
    "model = BiLSTMNER(vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_tag_indices_from_scores\n",
    "from metrics import MetricsHandler\n",
    "import torch.optim as optim\n",
    "\n",
    "labels_str = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n",
    "labels_int = list(range(len(labels_str)))\n",
    "train_metrics = MetricsHandler(labels_int)\n",
    "dev_metrics = MetricsHandler(labels_int)\n",
    "test_metrics = MetricsHandler(labels_int)\n",
    "\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, dev_loader, optimizer, loss_func, train_metrics, dev_metrics, num_epochs=5, device=None, train_dataset=None, dev_dataset=None, **kwargs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    dev_f1_scores = []  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        train_metrics = MetricsHandler(labels_int)  \n",
    "        \n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)  # (batch_size, seq_len, tagset_size)\n",
    "\n",
    "            output = output.view(-1, tagset_size)\n",
    "            targets_flat = targets.view(-1)\n",
    "\n",
    "            loss = loss_func(output, targets_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predictions = get_tag_indices_from_scores(output.detach().cpu().numpy())\n",
    "            train_metrics.update(predictions, targets_flat.cpu().numpy())\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_metrics.collect()\n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        losses.append(epoch_loss)\n",
    "        train_f1 = train_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        \n",
    "        dev_loss, dev_metrics = evaluate_model(model, dev_loader, loss_func, dev_metrics, device=device, dataset=dev_dataset)\n",
    "        dev_metrics.collect()\n",
    "        dev_f1 = dev_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "        dev_f1_scores.append(dev_f1)  # 记录开发集F-1分数\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Train F1: {train_f1:.4f}, Dev F1: {dev_f1:.4f}\")\n",
    "\n",
    "    return model, train_metrics, dev_metrics, losses, dev_f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def beam_search_decode(tag_scores, beam_width, tagset_size):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "        tag_scores: 模型输出的标签得分 (seq_len, tagset_size)\n",
    "        beam_width: beam宽度\n",
    "        tagset_size: 标签集大小\n",
    "    返回:\n",
    "        best_sequence: 得分最高的标签序列（列表）\n",
    "    \"\"\"\n",
    "    seq_len = tag_scores.size(0)\n",
    "    tag_scores = F.log_softmax(tag_scores, dim=-1) \n",
    "\n",
    "    beams = [(0.0, [])]\n",
    "    for t in range(seq_len):\n",
    "        all_candidates = []\n",
    "        for score, seq in beams:\n",
    "            curr_scores = tag_scores[t]  # (tagset_size,)\n",
    "            for tag in range(tagset_size):\n",
    "                new_score = score + curr_scores[tag].item()\n",
    "                new_seq = seq + [tag]\n",
    "                all_candidates.append((new_score, new_seq))\n",
    "        \n",
    "        all_candidates = sorted(all_candidates, key=lambda x: x[0], reverse=True)\n",
    "        beams = all_candidates[:beam_width]\n",
    "    \n",
    "    best_score, best_sequence = beams[0]\n",
    "    return best_sequence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_func, eval_metrics, device=None, dataset=None, beam_width=None):\n",
    "    model.eval()\n",
    "    eval_metrics = MetricsHandler(labels_int)\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if len(inputs.size()) == 1:  # 如果是1维，添加batch维度\n",
    "                inputs = inputs.unsqueeze(0)  # (seq_len,) -> (1, seq_len)\n",
    "            batch_size, seq_len = inputs.size()\n",
    "\n",
    "            output = model(inputs)  # (batch_size, seq_len, tagset_size)\n",
    "\n",
    "            output_flat = output.view(-1, tagset_size)\n",
    "            targets_flat = targets.view(-1)\n",
    "            loss = loss_func(output_flat, targets_flat)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            if beam_width is not None:\n",
    "                # 使用beam search\n",
    "                for i in range(batch_size):\n",
    "                    pred_seq = beam_search_decode(output[i], beam_width, tagset_size)  # 返回列表\n",
    "                    target_seq = targets[i].cpu().tolist() if len(targets.size()) > 1 else targets.cpu().tolist()\n",
    "                    all_preds.append(pred_seq)\n",
    "                    all_targets.append(target_seq)\n",
    "            else:\n",
    "                # 使用greedy search\n",
    "                predictions = get_tag_indices_from_scores(output_flat.cpu().numpy())\n",
    "                eval_metrics.update(predictions, targets_flat.cpu().numpy())\n",
    "\n",
    "    if beam_width is not None:\n",
    "        eval_metrics = MetricsHandler(labels_int)\n",
    "        for preds, targets in zip(all_preds, all_targets):\n",
    "            if not isinstance(preds, list) or not isinstance(targets, list):\n",
    "                raise TypeError(f\"preds or targets is not a list: preds={preds}, targets={targets}\")\n",
    "            if len(preds) != len(targets):\n",
    "                raise ValueError(f\"Prediction length {len(preds)} does not match target length {len(targets)}\")\n",
    "            eval_metrics.update(preds, targets)\n",
    "    \n",
    "    eval_loss = total_loss / len(dataset) if dataset is not None else total_loss\n",
    "    return eval_loss, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已从 bilstm_ner_model.pth 加载训练好的模型参数\n",
      "\n",
      "使用Greedy Search评估测试集（第3步结果）：\n",
      "Test F1 Score (Greedy Search): 0.8245\n",
      "\n",
      "使用Beam Search (beam_width=100)评估测试集：\n",
      "Test F1 Score (Beam Search): 0.8245\n",
      "\n",
      "性能差异 (Beam - Greedy): 0.0000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BiLSTMNER(vocab_size, embedding_dim, hidden_dim, tagset_size, embedding_matrix)\n",
    "model = model.to(device)\n",
    "\n",
    "model_path = \"bilstm_ner_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print(f\"已从 {model_path} 加载训练好的模型参数\")\n",
    "\n",
    "# 测试集评估：greedy search（与第3步对比）\n",
    "print(\"\\n使用Greedy Search评估测试集（第3步结果）：\")\n",
    "test_loss_greedy, test_metrics_greedy = evaluate_model(model, test_loader, loss_func, test_metrics, \n",
    "                                                       device=device, dataset=test_dataset, beam_width=None)\n",
    "test_metrics_greedy.collect()\n",
    "test_f1_greedy = test_metrics_greedy.get_metrics()[\"F1-score\"][-1]\n",
    "print(f\"Test F1 Score (Greedy Search): {test_f1_greedy:.4f}\")\n",
    "\n",
    "# 测试集评估：beam search\n",
    "beam_width = 100  \n",
    "print(f\"\\n使用Beam Search (beam_width={beam_width})评估测试集：\")\n",
    "test_loss_beam, test_metrics_beam = evaluate_model(model, test_loader, loss_func, test_metrics, \n",
    "                                                   device=device, dataset=test_dataset, beam_width=beam_width)\n",
    "test_metrics_beam.collect()\n",
    "test_f1_beam = test_metrics_beam.get_metrics()[\"F1-score\"][-1]\n",
    "print(f\"Test F1 Score (Beam Search): {test_f1_beam:.4f}\")\n",
    "\n",
    "print(f\"\\n性能差异 (Beam - Greedy): {test_f1_beam - test_f1_greedy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上评估\n",
    "print(\"开始测试...\")\n",
    "test_loss, test_metrics = evaluate_model(\n",
    "    model, test_loader, loss_func, test_metrics, \n",
    "    device=device, dataset=test_dataset\n",
    ")\n",
    "test_metrics.collect()\n",
    "test_f1 = test_metrics.get_metrics()[\"F1-score\"][-1]\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Other Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
