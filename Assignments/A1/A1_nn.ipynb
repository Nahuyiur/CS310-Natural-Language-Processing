{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 1. Neural Text Classification\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本分词器\n",
    "def basic_tokenizer(text):\n",
    "    # 仅保留中文字符\n",
    "    tokens=re.findall(r'[\\u4e00-\\u9fa5]',text)\n",
    "    return tokens\n",
    "\n",
    "# 改进分词器\n",
    "def improved_tokenizer(text):\n",
    "    reg=r'[\\u4e00-\\u9fa5]|[a-zA-Z]+|[0-9]+|[^\\w\\s]'\n",
    "    tokens = re.findall(reg, text)\n",
    "    return tokens\n",
    "\n",
    "# 读取数据\n",
    "def build_vocab_from_file(file_path, tokenizer):\n",
    "    word_freq = defaultdict(int)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            sentence = data['sentence']\n",
    "            tokens = tokenizer(sentence)\n",
    "            for token in tokens:\n",
    "                word_freq[token] += 1  # 更新每个token的出现频率\n",
    "    \n",
    "    # 构建词汇表，初始化未知词为 <unk>\n",
    "    vocab = {'<unk>': 0}\n",
    "    vocab.update({token: idx + 1 for idx, (token, freq) in enumerate(word_freq.items())})\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab_from_file('train.jsonl', improved_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, vocab):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self._prepare_data(file_path)\n",
    "\n",
    "    def _prepare_data(self, file_path):\n",
    "        self.data = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                sentence = item['sentence']\n",
    "        \n",
    "                label = item['label'][0]  # 标签是一个长度为1的列表\n",
    "\n",
    "                tokens = self.tokenizer(sentence)\n",
    "                token_ids = list(map(lambda token: self.vocab.get(token, self.vocab['<unk>']), tokens))  # 使用 map 处理 token\n",
    "\n",
    "                self.data.append((token_ids, label))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = TextDataset('train.jsonl', improved_tokenizer, vocab)\n",
    "test_dataset = TextDataset('test.jsonl', improved_tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量处理数据\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    \n",
    "    for tokens, label in batch:\n",
    "        label_list.append(label)\n",
    "        token_ids = torch.tensor(tokens, dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(offsets[-1] + token_ids.size(0))\n",
    "    \n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    token_ids = torch.cat(token_ids_list, dim=0)\n",
    "    offsets = torch.tensor(offsets[:-1], dtype=torch.int64)\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # 用EmbeddingBag层进行词嵌入\n",
    "        self.embedding = torch.nn.EmbeddingBag(vocab_size, embed_dim,sparse=False)\n",
    "\n",
    "        #用全连接层（2个隐藏层）\n",
    "        #用torch.nn.Sequential 实现\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_class)\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5 # 初始化权重的范围\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        # 初始化fc层权重和偏置\n",
    "        for layer in self.fc:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "                layer.bias.data.zero_()\n",
    "                \n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        out = self.fc(embedded)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_dataset)\n",
    "num_class = len(set([label for _, label in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64  # embedding size\n",
    "\n",
    "# 实例化模型\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "\n",
    "EPOCHS = 10 \n",
    "LR = 5 \n",
    "BATCH_SIZE = 8 \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader): \n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids, offsets)\n",
    "\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            print('token_ids: ', token_ids)\n",
    "            print('offsets: ', offsets)\n",
    "            raise\n",
    "\n",
    "        # Backward propagation, grad clipping, and optimization\n",
    "        loss.backward()  # Backpropagate\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # Calculate correct prediction in current batch\n",
    "        pred = torch.argmax(output, dim=1, keepdim=False)  # Apply argmax on output logits\n",
    "        correct = (pred == labels).sum().item()  # Calculate correct predictions\n",
    "        total_acc += correct  # Accumulate correct predictions\n",
    "\n",
    "        total_count += labels.size(0)  # Increment total count\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    preds = []  \n",
    "    labelset = []  \n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        if offsets[-1] > len(token_ids):\n",
    "            print(idx)\n",
    "            print(\"Labels:\", labels)\n",
    "            print(\"Token IDs:\", token_ids)\n",
    "            print(\"Offsets:\", offsets)\n",
    "        output = model(token_ids, offsets)  # 前向传播，获取模型输出\n",
    "        loss = criterion(output, labels)  \n",
    "\n",
    "        predicted_labels = torch.argmax(output, dim=1)  \n",
    "        correct = (predicted_labels == labels).int().sum().item()  \n",
    "        total_acc += correct  \n",
    "        total_count += labels.size(0)\n",
    "\n",
    "        preds.extend(predicted_labels.cpu().numpy())\n",
    "        labelset.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = total_acc / total_count  \n",
    "    return accuracy, preds, labelset\n",
    "\n",
    "def calculate_metrics(all_labels, all_preds):\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    TP = np.sum((all_labels == 1) & (all_preds == 1))\n",
    "    FP = np.sum((all_labels == 0) & (all_preds == 1))\n",
    "    FN = np.sum((all_labels == 1) & (all_preds == 0))\n",
    "    TN = np.sum((all_labels == 0) & (all_preds == 0))\n",
    "\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练和测试数据\n",
    "train_iter = iter(train_dataset)\n",
    "test_iter = iter(test_dataset)\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1506 batches | accuracy    0.672\n",
      "| epoch   1 |   400/ 1506 batches | accuracy    0.698\n",
      "| epoch   1 |   600/ 1506 batches | accuracy    0.693\n",
      "| epoch   1 |   800/ 1506 batches | accuracy    0.695\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.704\n",
      "| epoch   1 |  1200/ 1506 batches | accuracy    0.686\n",
      "| epoch   1 |  1400/ 1506 batches | accuracy    0.676\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.78s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 1506 batches | accuracy    0.685\n",
      "| epoch   2 |   400/ 1506 batches | accuracy    0.707\n",
      "| epoch   2 |   600/ 1506 batches | accuracy    0.691\n",
      "| epoch   2 |   800/ 1506 batches | accuracy    0.689\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.693\n",
      "| epoch   2 |  1200/ 1506 batches | accuracy    0.679\n",
      "| epoch   2 |  1400/ 1506 batches | accuracy    0.691\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.76s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 1506 batches | accuracy    0.704\n",
      "| epoch   3 |   400/ 1506 batches | accuracy    0.688\n",
      "| epoch   3 |   600/ 1506 batches | accuracy    0.705\n",
      "| epoch   3 |   800/ 1506 batches | accuracy    0.700\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.680\n",
      "| epoch   3 |  1200/ 1506 batches | accuracy    0.702\n",
      "| epoch   3 |  1400/ 1506 batches | accuracy    0.679\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.81s | valid accuracy    0.467 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 1506 batches | accuracy    0.700\n",
      "| epoch   4 |   400/ 1506 batches | accuracy    0.710\n",
      "| epoch   4 |   600/ 1506 batches | accuracy    0.718\n",
      "| epoch   4 |   800/ 1506 batches | accuracy    0.728\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.716\n",
      "| epoch   4 |  1200/ 1506 batches | accuracy    0.724\n",
      "| epoch   4 |  1400/ 1506 batches | accuracy    0.721\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.82s | valid accuracy    0.707 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 1506 batches | accuracy    0.709\n",
      "| epoch   5 |   400/ 1506 batches | accuracy    0.721\n",
      "| epoch   5 |   600/ 1506 batches | accuracy    0.718\n",
      "| epoch   5 |   800/ 1506 batches | accuracy    0.714\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.728\n",
      "| epoch   5 |  1200/ 1506 batches | accuracy    0.733\n",
      "| epoch   5 |  1400/ 1506 batches | accuracy    0.717\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.75s | valid accuracy    0.707 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   200/ 1506 batches | accuracy    0.717\n",
      "| epoch   6 |   400/ 1506 batches | accuracy    0.726\n",
      "| epoch   6 |   600/ 1506 batches | accuracy    0.729\n",
      "| epoch   6 |   800/ 1506 batches | accuracy    0.726\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.726\n",
      "| epoch   6 |  1200/ 1506 batches | accuracy    0.701\n",
      "| epoch   6 |  1400/ 1506 batches | accuracy    0.726\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.76s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   200/ 1506 batches | accuracy    0.725\n",
      "| epoch   7 |   400/ 1506 batches | accuracy    0.714\n",
      "| epoch   7 |   600/ 1506 batches | accuracy    0.737\n",
      "| epoch   7 |   800/ 1506 batches | accuracy    0.705\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.715\n",
      "| epoch   7 |  1200/ 1506 batches | accuracy    0.731\n",
      "| epoch   7 |  1400/ 1506 batches | accuracy    0.712\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.75s | valid accuracy    0.716 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   200/ 1506 batches | accuracy    0.717\n",
      "| epoch   8 |   400/ 1506 batches | accuracy    0.723\n",
      "| epoch   8 |   600/ 1506 batches | accuracy    0.714\n",
      "| epoch   8 |   800/ 1506 batches | accuracy    0.730\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.716\n",
      "| epoch   8 |  1200/ 1506 batches | accuracy    0.711\n",
      "| epoch   8 |  1400/ 1506 batches | accuracy    0.725\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.75s | valid accuracy    0.716 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   200/ 1506 batches | accuracy    0.705\n",
      "| epoch   9 |   400/ 1506 batches | accuracy    0.724\n",
      "| epoch   9 |   600/ 1506 batches | accuracy    0.734\n",
      "| epoch   9 |   800/ 1506 batches | accuracy    0.719\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.716\n",
      "| epoch   9 |  1200/ 1506 batches | accuracy    0.729\n",
      "| epoch   9 |  1400/ 1506 batches | accuracy    0.718\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.76s | valid accuracy    0.716 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   200/ 1506 batches | accuracy    0.721\n",
      "| epoch  10 |   400/ 1506 batches | accuracy    0.711\n",
      "| epoch  10 |   600/ 1506 batches | accuracy    0.739\n",
      "| epoch  10 |   800/ 1506 batches | accuracy    0.719\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.710\n",
      "| epoch  10 |  1200/ 1506 batches | accuracy    0.725\n",
      "| epoch  10 |  1400/ 1506 batches | accuracy    0.700\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.75s | valid accuracy    0.716 \n",
      "-----------------------------------------------------------\n",
      "Accuracy: 0.7435\n",
      "Precision: 0.5283\n",
      "Recall: 0.1647\n",
      "F1 Score: 0.2511\n"
     ]
    }
   ],
   "source": [
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val,_,_ = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "test_accuracy, test_preds, test_labels = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "\n",
    "num_classes = len(set(test_labels))\n",
    "accuracy,precision, recall, f1 = calculate_metrics(test_labels, test_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1506 batches | accuracy    0.675\n",
      "| epoch   1 |   400/ 1506 batches | accuracy    0.672\n",
      "| epoch   1 |   600/ 1506 batches | accuracy    0.675\n",
      "| epoch   1 |   800/ 1506 batches | accuracy    0.682\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.681\n",
      "| epoch   1 |  1200/ 1506 batches | accuracy    0.689\n",
      "| epoch   1 |  1400/ 1506 batches | accuracy    0.706\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  1.39s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 1506 batches | accuracy    0.688\n",
      "| epoch   2 |   400/ 1506 batches | accuracy    0.671\n",
      "| epoch   2 |   600/ 1506 batches | accuracy    0.688\n",
      "| epoch   2 |   800/ 1506 batches | accuracy    0.685\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.689\n",
      "| epoch   2 |  1200/ 1506 batches | accuracy    0.683\n",
      "| epoch   2 |  1400/ 1506 batches | accuracy    0.686\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  1.42s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 1506 batches | accuracy    0.672\n",
      "| epoch   3 |   400/ 1506 batches | accuracy    0.699\n",
      "| epoch   3 |   600/ 1506 batches | accuracy    0.678\n",
      "| epoch   3 |   800/ 1506 batches | accuracy    0.676\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.699\n",
      "| epoch   3 |  1200/ 1506 batches | accuracy    0.695\n",
      "| epoch   3 |  1400/ 1506 batches | accuracy    0.667\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  1.41s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 1506 batches | accuracy    0.681\n",
      "| epoch   4 |   400/ 1506 batches | accuracy    0.684\n",
      "| epoch   4 |   600/ 1506 batches | accuracy    0.697\n",
      "| epoch   4 |   800/ 1506 batches | accuracy    0.675\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.686\n",
      "| epoch   4 |  1200/ 1506 batches | accuracy    0.679\n",
      "| epoch   4 |  1400/ 1506 batches | accuracy    0.681\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  1.41s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 1506 batches | accuracy    0.674\n",
      "| epoch   5 |   400/ 1506 batches | accuracy    0.687\n",
      "| epoch   5 |   600/ 1506 batches | accuracy    0.681\n",
      "| epoch   5 |   800/ 1506 batches | accuracy    0.663\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.693\n",
      "| epoch   5 |  1200/ 1506 batches | accuracy    0.684\n",
      "| epoch   5 |  1400/ 1506 batches | accuracy    0.695\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  1.40s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   200/ 1506 batches | accuracy    0.688\n",
      "| epoch   6 |   400/ 1506 batches | accuracy    0.688\n",
      "| epoch   6 |   600/ 1506 batches | accuracy    0.683\n",
      "| epoch   6 |   800/ 1506 batches | accuracy    0.680\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.683\n",
      "| epoch   6 |  1200/ 1506 batches | accuracy    0.668\n",
      "| epoch   6 |  1400/ 1506 batches | accuracy    0.684\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  1.45s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   200/ 1506 batches | accuracy    0.675\n",
      "| epoch   7 |   400/ 1506 batches | accuracy    0.689\n",
      "| epoch   7 |   600/ 1506 batches | accuracy    0.686\n",
      "| epoch   7 |   800/ 1506 batches | accuracy    0.688\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.682\n",
      "| epoch   7 |  1200/ 1506 batches | accuracy    0.677\n",
      "| epoch   7 |  1400/ 1506 batches | accuracy    0.680\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  1.41s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   200/ 1506 batches | accuracy    0.679\n",
      "| epoch   8 |   400/ 1506 batches | accuracy    0.671\n",
      "| epoch   8 |   600/ 1506 batches | accuracy    0.685\n",
      "| epoch   8 |   800/ 1506 batches | accuracy    0.706\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.686\n",
      "| epoch   8 |  1200/ 1506 batches | accuracy    0.674\n",
      "| epoch   8 |  1400/ 1506 batches | accuracy    0.667\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  1.40s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   200/ 1506 batches | accuracy    0.688\n",
      "| epoch   9 |   400/ 1506 batches | accuracy    0.697\n",
      "| epoch   9 |   600/ 1506 batches | accuracy    0.696\n",
      "| epoch   9 |   800/ 1506 batches | accuracy    0.698\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.647\n",
      "| epoch   9 |  1200/ 1506 batches | accuracy    0.688\n",
      "| epoch   9 |  1400/ 1506 batches | accuracy    0.663\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  1.39s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   200/ 1506 batches | accuracy    0.678\n",
      "| epoch  10 |   400/ 1506 batches | accuracy    0.675\n",
      "| epoch  10 |   600/ 1506 batches | accuracy    0.675\n",
      "| epoch  10 |   800/ 1506 batches | accuracy    0.682\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.677\n",
      "| epoch  10 |  1200/ 1506 batches | accuracy    0.684\n",
      "| epoch  10 |  1400/ 1506 batches | accuracy    0.701\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  1.41s | valid accuracy    0.711 \n",
      "-----------------------------------------------------------\n",
      "Accuracy: 0.7097\n",
      "Precision: 0.2564\n",
      "Recall: 0.0588\n",
      "F1 Score: 0.0957\n"
     ]
    }
   ],
   "source": [
    "# 使用jieba分词器\n",
    "def jieba_tokenizer(text):\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "vocab = build_vocab_from_file('train.jsonl', jieba_tokenizer)\n",
    "# print('Vocabulary size:', len(vocab))\n",
    "\n",
    "train_dataset = TextDataset('train.jsonl', jieba_tokenizer, vocab)\n",
    "test_dataset = TextDataset('test.jsonl', jieba_tokenizer, vocab)\n",
    "\n",
    "train_loader=DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader=DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "model=TextClassificationModel(len(vocab), embed_dim=64, num_class=2).to(device)\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accu_val,_,_ = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "test_accuracy, test_preds, test_labels = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "num_classes = len(set(test_labels))\n",
    "accuracy,precision, recall, f1 = calculate_metrics(test_labels, test_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
